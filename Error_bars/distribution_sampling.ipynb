{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution sampling\n",
    "\n",
    "Caelen: each param of a Dirichlet is one of the entries in a CM, each param of multinomial is sampled from Dirichlet.\n",
    "\n",
    "Steps (don't have to do every single time tho):\n",
    "1. Start docker container for whatever db u want\n",
    "2. Run store_expanded_labeled_trips to set the expanded_labeled_trips variable\n",
    "3. Generate mode maps for user mode labels in ur dataset somehow...\n",
    "4. Run the \"get data\" cell in this notebook\n",
    "\n",
    "Then run:\n",
    "- sampling(CM to sample, n) \n",
    "- actual_given_sensed_CM(list of simulated CMs) \n",
    "- count_estimate(CM to test, probabilities) or distance_estimate(CM to test, probabilities, OS)\n",
    "\n",
    "## CHANGES MADE after talking to Wen :)\n",
    "1. Made two functions, where  the inputs are just the validation set and the training CM. These are called find_counts and find_distances. Use these!\n",
    "2. Fixed the estimates functions to take validation set (list of predictions for each mode) instead of a CM.\n",
    "3. Moved all the necessary cells to run to the top, under \"RUN THESE!\"\n",
    "4. Moved all the examples of using the functions under \"EXAMPLES\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN THESE!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling and probability fcns for both counts and distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling and probability calculation fcns which you need to do for both counts and distances.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "'''\n",
    "Sample a CM (as a DF) and create n # of CMs based on it.\n",
    "input: a CM to sample, number of times to sample\n",
    "output: a list of n CMs\n",
    "'''\n",
    "def sampling(samplingCM, n):\n",
    "    # dirichlet sampling\n",
    "    v = [] # from the CM, going left to right by row\n",
    "    for row in samplingCM.index:\n",
    "        for col in samplingCM.columns:\n",
    "            v.append(samplingCM.loc[row][col])\n",
    "    a = np.ones(samplingCM.size)\n",
    "\n",
    "    rng = default_rng()\n",
    "    dirichlet_samples = rng.dirichlet((v+a), n)\n",
    "\n",
    "    # multinomial sampling\n",
    "    multinomial_samples = []\n",
    "    n_trips = 0\n",
    "    for col in samplingCM:\n",
    "        n_trips += samplingCM[col].sum()\n",
    "\n",
    "    for params in dirichlet_samples:\n",
    "        s = rng.multinomial(n_trips, params)\n",
    "        multinomial_samples.append(s)\n",
    "\n",
    "    # put each of these into their own CM, same dimensions as samplingCM (do row by row)\n",
    "    output_CMs = []\n",
    "    for samples in multinomial_samples:\n",
    "        samples2D = np.reshape(samples, (len(samplingCM.index), len(samplingCM.columns)))\n",
    "        outputCM = pd.DataFrame(samples2D, columns = samplingCM.columns, index = samplingCM.index)\n",
    "        output_CMs.append(outputCM)\n",
    "    return output_CMs\n",
    "\n",
    "'''\n",
    "Finding P(actual|sensed) by dividing each cell by column sum, for a list of DFs\n",
    "input: list of DFs of values\n",
    "output: list of DF of P(actual|sensed)\n",
    "'''\n",
    "def actual_given_sensed_CM(list_of_value_CMs):\n",
    "    actual_given_sensed = []\n",
    "    for cm in list_of_value_CMs:\n",
    "        probs = cm.div(cm.sum(axis=0), axis=1)\n",
    "        actual_given_sensed.append(probs)\n",
    "    return actual_given_sensed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance estimation and variance calculation fcn. very similar to the count fcn, not sure whether to combine them or not.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Finding estimated values and variances FOR DISTANCES.\n",
    "input: \n",
    "    predicted_distances: a dictionary {\"mode1\": # of trips predicted in mode1...}\n",
    "    actual_given_sensed: list of DFs, which have probabilities in each cell\n",
    "    os: which OS we're using, either \"ios\" or \"android\"\n",
    "output: a single estimated distance for each mode, and a single estimated variance for each mode. \n",
    "    (prints some other interesting stuff out too)\n",
    "'''\n",
    "def distance_estimate(predicted_distances, actual_given_sensed, os):\n",
    "    os_unit_info = pd.read_csv(r'unit_distance_MCS.csv')\n",
    "\n",
    "    # adjusting using os unit info\n",
    "    adjusted_predicted_distances = {}\n",
    "    for mode in predicted_distances.index:\n",
    "        adjusted_predicted_distances[mode] = predicted_distances[mode] * os_unit_info[os][0]\n",
    "\n",
    "    # find expected counts based on each actual_given_sensed CM and predicted counts\n",
    "    expected_counts = [] #list of dfs (one df per cm)\n",
    "    for df in actual_given_sensed:\n",
    "        expected_value = df.mul(adjusted_predicted_distances, axis = 'columns') # multiply row by row\n",
    "        expected_value = expected_value.sum(axis='columns') # sum of each row\n",
    "        expected_counts.append(expected_value)\n",
    "    \n",
    "    # average expected values: concat dfs and find mean\n",
    "    all_expected = pd.concat(expected_counts, axis='columns')\n",
    "    average_ev = all_expected.mean(axis='columns')\n",
    "\n",
    "    #VARIANCES\n",
    "    df_list = []\n",
    "    for df in actual_given_sensed:\n",
    "        df_list.append(df.to_numpy())\n",
    "\n",
    "    # variance of each cell in prob CMs    \n",
    "    cell_variance = np.square(pd.DataFrame(np.dstack((df_list)).std(axis=2), columns = actual_given_sensed[0].columns, index = actual_given_sensed[0].index)) \n",
    "    \n",
    "    # multiply each row of cell variances by the row of L_i^2\n",
    "    adjusted_predicted_distances = pd.DataFrame([adjusted_predicted_distances])\n",
    "    n_squared = np.square(adjusted_predicted_distances) #row of L^2s\n",
    "    variance1 = cell_variance.mul(n_squared.values, axis = 'columns') # E(L_mode)^2 V(p)\n",
    "    \n",
    "    # extra variance term since distance has its own uncertainty\n",
    "    avg_actual_given_sensed = pd.DataFrame(np.dstack((df_list)).mean(axis=2), columns = actual_given_sensed[0].columns, index = actual_given_sensed[0].index)\n",
    "    dist_variance = np.square(pd.Series(predicted_distances)) * os_unit_info[os][1] # row of L_i^2s\n",
    "    variance2 = dist_variance.mul(np.square(avg_actual_given_sensed)) # E(p)^2*V(L_mode)]\n",
    "\n",
    "    # sum up rows\n",
    "    variance = variance1.add(variance2)\n",
    "    variance = variance.sum(axis='columns')\n",
    "\n",
    "    # diff = pd.Series(average_ev).T.subtract(pd.Series(predicted_distances)).T #expected value - actual value\n",
    "\n",
    "    # count_results = pd.DataFrame({\"EV\": average_ev, \n",
    "    #                             \"actual\": pd.Series(predicted_distances), \n",
    "    #                             \"diff\":diff,\n",
    "    #                             \"variance\":variance,\n",
    "    #                             \"stdev from actual\":diff.div(np.sqrt(variance), axis='rows')}) #diff/sqrt(variance)\n",
    "    # print(count_results)\n",
    "    # print(\"\\nPredicted:\\n\", predicted_distances.T)\n",
    "    return (average_ev, variance)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count estimate and variance calculation function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "Finding estimated values and variances FOR COUNTS.\n",
    "input:\n",
    "    predicted_counts: a dictionary {\"mode1\": # of trips predicted in mode1...}\n",
    "    actual_given_sensed: list of DFs, which have probabilities in each cell\n",
    "output: a single estimated count for each mode, and a single estimated variance for each mode count. \n",
    "    (prints some other interesting stuff out too)\n",
    "'''\n",
    "def count_estimate(predicted_counts, actual_given_sensed):\n",
    "    # find expected counts based on each actual_given_sensed CM and predicted counts\n",
    "    expected_counts = [] #list of dfs (one df per cm)\n",
    "    for df in actual_given_sensed:\n",
    "        expected_value = df.mul(pd.Series(predicted_counts), axis = 'columns') # multiply row by row\n",
    "        expected_value = expected_value.sum(axis='columns') # sum of each row\n",
    "        expected_counts.append(expected_value)\n",
    "\n",
    "    # average expected values: concat dfs and find mean\n",
    "    all_expected = pd.concat(expected_counts, axis='columns')\n",
    "    average_ev = all_expected.mean(axis='columns')\n",
    "\n",
    "    #VARIANCES\n",
    "    # variance of each cell\n",
    "    df_list = []\n",
    "    for df in actual_given_sensed:\n",
    "        df_list.append(df.to_numpy())\n",
    "    cell_variance = np.square(pd.DataFrame(np.dstack((df_list)).std(axis=2), columns = actual_given_sensed[0].columns, index = actual_given_sensed[0].index)) \n",
    "\n",
    "    # multiply each row of cell variances by  the row of n_i^2\n",
    "    predicted_counts = pd.DataFrame([predicted_counts])\n",
    "    n_squared = np.square(predicted_counts) #row of n^2s\n",
    "    n2_times_var = cell_variance.mul(n_squared.values, axis = 'columns')\n",
    "\n",
    "    # sum up rows\n",
    "    variance = n2_times_var.sum(axis='columns')\n",
    "\n",
    "    # diff = pd.Series(average_ev).T.subtract(predicted_counts).T #expected value - actual value\n",
    "\n",
    "    # count_results = pd.DataFrame({\"EV\": average_ev, \n",
    "    #                             \"actual\": (predicted_counts),\n",
    "                                # \"diff\":diff,\n",
    "                                # \"stdev from actual\":diff.div(np.sqrt(variance), axis='rows')}) #diff/sqrt(variance)\n",
    "    # print(count_results)\n",
    "    # print(\"\\nPredicted:\\n\", predicted_counts.T)\n",
    "\n",
    "    return (average_ev, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find counts\n",
    "def find_counts(trainingCM, data):\n",
    "    countSamples = sampling(trainingCM, 2000) # you can change 2000 to anything!\n",
    "    countProbs = actual_given_sensed_CM(countSamples)\n",
    "    countEstimates = count_estimate(data, countProbs)\n",
    "    return countEstimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find distances\n",
    "def find_distances(trainingCM, data, os):\n",
    "    distanceSamples = sampling(trainingCM, 2000) # you can change 2000 to anything!\n",
    "    distanceProbs = actual_given_sensed_CM(distanceSamples)\n",
    "    distanceEstimates = distance_estimate(data, distanceProbs, os)\n",
    "    return distanceEstimates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CM-creation funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for taking raw CBC CO trip data to cleaned CMs\n",
    "# add the fcns here to confusion_matrix_handling.py!!!\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helper_functions as helper\n",
    "import confusion_matrix_handling as cm_handling\n",
    "\n",
    "'''\n",
    "Loop through a DF of trips and tally up label pairs. Pairs are made of 'primary_mode' and 'mode_confirm' labels.\n",
    "    (primary_mode column comes from get_primary_modes fcn in helper_functions.py)\n",
    "    Will match either by counts or by distance.\n",
    "Input: DF of raw trip data (data with a buncha extra info, one row per trip)\n",
    "Output: CM of pairs and their counts or distances (as a DF).\n",
    "'''\n",
    "def pair_matching(df, by_distances=False):\n",
    "    # make columns of sensed labels (primary_mode)\n",
    "    columns = {} # looks like {sensed_label: {user_label: count, user_label: count...}}\n",
    "    value = 1 # default is by counts, where we add one each time\n",
    "    for index, row in df.iterrows():\n",
    "        if by_distances: # if set to true, instead of adding one for each pair, we add distances\n",
    "            value = row['distance']\n",
    "        if row['primary_mode'] not in columns: # if sensed label not added as column yet\n",
    "            columns[row['primary_mode']] = {} # make column\n",
    "            columns[row['primary_mode']][row['mode_confirm']] = value\n",
    "        elif row['mode_confirm'] not in columns[row['primary_mode']]: #if user label not in sensed label column\n",
    "            columns[row['primary_mode']][row['mode_confirm']] = value\n",
    "        else: # else [sensed label, user label] pair already there\n",
    "            columns[row['primary_mode']][row['mode_confirm']] += value\n",
    "        \n",
    "    pairs = pd.DataFrame(columns)\n",
    "    return pairs\n",
    "\n",
    "'''\n",
    "Consolidate user labels (row labels, aka DF index labels) of a user_label, sensed_label CM that are basically the same by using some mode map dictionary.\n",
    "    If rows have the same label after mapping, they're combined by addition.\n",
    "    If rows have no user label, they're dropped.\n",
    "    If remove_unknown is true, unknown labels are also dropped.\n",
    "Input: DF, mapping dictionary (with the form {user label: standardized label})\n",
    "Output: DF with new index labels\n",
    "'''\n",
    "def map_labels(df, map, remove_unknown=True):\n",
    "    # do mapping\n",
    "    renamed_pairs = df.rename(index=map)\n",
    "    consolidated_pairs = renamed_pairs.groupby(level=0).aggregate(['sum'])\n",
    "\n",
    "    # remove the annoying \"sum\" part of the label that appears after aggregate\n",
    "    consolidated_pairs.columns=consolidated_pairs.columns.droplevel(1)\n",
    "    \n",
    "    # remove rows where user label is not in map if set\n",
    "    if remove_unknown:\n",
    "        for index, data in consolidated_pairs.iterrows():\n",
    "            if index not in map:\n",
    "                consolidated_pairs = consolidated_pairs.drop(labels = [index], axis = 0)\n",
    "    return consolidated_pairs\n",
    "\n",
    "'''\n",
    "Add missing rows or columns to a DF according to a list of other DFs. New rows/cols filled with all zeroes.\n",
    "Input: list of model DFs, DF to modify\n",
    "Output: DF with filled in rows and columns\n",
    "'''\n",
    "def fill_in(inputCMs, badCM):\n",
    "    wanted_columns = []\n",
    "    wanted_rows = []\n",
    "    for inputCM in inputCMs:\n",
    "        for col in inputCM.columns:\n",
    "            if col not in wanted_columns:\n",
    "                wanted_columns.append(col)\n",
    "        for row in inputCM.index:\n",
    "            if row not in wanted_rows:\n",
    "                wanted_rows.append(row)\n",
    "                \n",
    "    for col in wanted_columns:\n",
    "        if col not in badCM.columns:\n",
    "            badCM[col] = np.zeros(len(badCM.index))\n",
    "    for row in wanted_rows:\n",
    "        if row not in badCM.index:\n",
    "            badCM.loc[row] = np.zeros(len(badCM.columns))\n",
    "    return badCM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting CanBikeCO trip data into CMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 93 trips with no sensed sections.\n",
      "Dropping user labeled AIR trips and trips with no OS.\n",
      "Also dropping trips labeled as not a trip and trips with mode_confirm of nan.\n",
      "                               car      walking        bus    bicycling  \\\n",
      "Bikeshare                14.080071    28.123487   0.000000    90.813872   \n",
      "Bus                    2119.844174   844.631202  50.693043   206.844883   \n",
      "E-car, drove alone        0.000000     4.050190   0.000000     0.000000   \n",
      "Free Shuttle             53.033213     8.721774   0.000000     1.891534   \n",
      "Gas Car, drove alone  26176.607588  2836.043327  25.776840   638.854047   \n",
      "Gas Car, with others  47759.367236  5406.811993  38.180835  1379.209095   \n",
      "Pilot ebike            9206.409708  3466.478592   3.856213  7384.268952   \n",
      "Regular Bike            704.403608   633.217658   8.981032  1292.084770   \n",
      "Scooter share             0.000000     4.659207   0.000000     0.000000   \n",
      "Skate board               0.000000     0.000000   0.000000     0.000000   \n",
      "Taxi/Uber/Lyft          285.415494    71.053504   0.000000    29.610642   \n",
      "Train                   172.383661   177.793097   0.000000    48.371663   \n",
      "Walk                    526.971197  2052.190454  17.259913   250.544147   \n",
      "\n",
      "                       air_or_hsr   no_sensed     subway       train  \n",
      "Bikeshare                0.000000    0.211578   0.000000    0.000000  \n",
      "Bus                      0.000000   16.427162   0.000000   58.380628  \n",
      "E-car, drove alone       0.000000    0.000000   0.000000    0.000000  \n",
      "Free Shuttle             0.000000    0.000000   0.000000    0.000000  \n",
      "Gas Car, drove alone    39.167115  803.762435   0.000000    0.000000  \n",
      "Gas Car, with others  1506.254441  804.642989   0.000000    7.056064  \n",
      "Pilot ebike             37.598751  538.791925   0.000000    0.000000  \n",
      "Regular Bike             0.000000  112.459071   0.000000    0.000000  \n",
      "Scooter share            0.000000    0.000000   0.000000    0.000000  \n",
      "Skate board              0.000000    0.000000   0.000000    0.000000  \n",
      "Taxi/Uber/Lyft           0.000000    2.510731   0.000000    0.000000  \n",
      "Train                    0.000000    0.000000  79.401147  122.442857  \n",
      "Walk                     1.382034   45.820372   0.000000    0.886342  \n",
      "                               car      walking    bicycling    no_sensed  \\\n",
      "Bikeshare                28.978960    24.769136    32.188102     0.000000   \n",
      "Bus                    3492.478446   316.666835    87.631480   318.036952   \n",
      "E-car, drove alone        0.000000     0.000000     0.000000     0.000000   \n",
      "Free Shuttle             97.996939    10.342667     4.565073     9.688586   \n",
      "Gas Car, drove alone  29925.160173  1634.763704   372.056496  1608.802280   \n",
      "Gas Car, with others  51557.440089  2307.252408   593.112439  2681.264738   \n",
      "Pilot ebike            4416.671860  1850.423233  5530.044467   561.195206   \n",
      "Regular Bike            215.859696   162.454745  1184.290796    48.627487   \n",
      "Scooter share            25.679474    38.076282    13.243006     4.264616   \n",
      "Skate board               3.515523     4.716632    16.000626     2.543864   \n",
      "Taxi/Uber/Lyft          846.100092   100.028249    20.811323   123.970252   \n",
      "Train                   404.376798     7.007207     5.701570   402.048111   \n",
      "Walk                    994.838204  2678.281961   219.951908    81.657945   \n",
      "\n",
      "                             bus       train      subway   air_or_hsr  \n",
      "Bikeshare               3.935394    0.000000    0.000000     0.000000  \n",
      "Bus                    77.125936   60.311131    0.000000  1620.047459  \n",
      "E-car, drove alone      0.000000    0.000000    0.000000     0.000000  \n",
      "Free Shuttle            0.585084    1.039703    0.000000     0.000000  \n",
      "Gas Car, drove alone  154.364844    0.000000    0.000000     0.000000  \n",
      "Gas Car, with others  136.094222  117.330100    0.000000  4119.146863  \n",
      "Pilot ebike            26.793060   43.497831    0.000000     0.000000  \n",
      "Regular Bike            1.192544   37.603833    0.000000     0.000000  \n",
      "Scooter share           5.974656    0.000000    0.000000    32.906139  \n",
      "Skate board             0.000000    0.000000    0.000000     0.000000  \n",
      "Taxi/Uber/Lyft          0.000000    0.000000    0.000000     0.000000  \n",
      "Train                  23.321068  506.371135  143.456246   391.306238  \n",
      "Walk                    5.701441    1.897807    0.000000  3475.809844  \n",
      "                               car      walking         bus    bicycling  \\\n",
      "Bikeshare                 2.295983    32.995879    0.000000    70.746980   \n",
      "Bus                    1347.450122   552.597004   15.281032   220.731982   \n",
      "E-car, drove alone        0.000000     0.000000    0.000000     0.000000   \n",
      "Free Shuttle             29.321462     2.590171    0.000000    20.678344   \n",
      "Gas Car, drove alone  23941.227708  2608.692104   50.924052   642.481143   \n",
      "Gas Car, with others  46321.722131  4939.809076  132.451035  1298.074905   \n",
      "Pilot ebike            8670.615048  2924.548392    9.553891  6233.768132   \n",
      "Regular Bike            411.577745   457.085707    0.960918  1168.699645   \n",
      "Scooter share             3.842482     2.148633    0.000000     0.680371   \n",
      "Skate board              99.021130     0.000000    0.000000     0.000000   \n",
      "Taxi/Uber/Lyft          693.728441    92.059749    0.000000    26.112027   \n",
      "Train                   246.642395   230.064986    0.000000    30.018076   \n",
      "Walk                    510.548743  1733.896051   15.209854   294.801897   \n",
      "\n",
      "                        no_sensed  air_or_hsr       train     subway  \n",
      "Bikeshare                0.000000    0.000000    0.000000   0.000000  \n",
      "Bus                     73.869822    0.000000   33.110551   0.000000  \n",
      "E-car, drove alone       0.000000    0.000000    0.000000   0.000000  \n",
      "Free Shuttle             0.000000    0.000000    0.000000   0.000000  \n",
      "Gas Car, drove alone   840.077955   59.028673    0.000000   0.000000  \n",
      "Gas Car, with others  1262.215751  494.610687    0.000000   0.000000  \n",
      "Pilot ebike            659.801309   68.319485    0.000000   0.000000  \n",
      "Regular Bike            78.240716    0.000000    0.000000   0.000000  \n",
      "Scooter share            0.000000    0.000000    0.000000   0.000000  \n",
      "Skate board              0.000000    0.000000    0.000000   0.000000  \n",
      "Taxi/Uber/Lyft           4.622094    0.000000    0.000000   0.000000  \n",
      "Train                    0.000000    0.000000  298.993865  27.949139  \n",
      "Walk                    31.180665    2.098764    0.000000   0.000000  \n",
      "                          walking           car    bicycling         bus  \\\n",
      "Bikeshare               20.375535     33.138828    29.213148    0.000000   \n",
      "Bus                    406.445548   2731.402791   148.488196   55.980588   \n",
      "E-car, drove alone       0.000000      0.000000     0.000000    0.000000   \n",
      "Free Shuttle             3.780839    195.554790     2.883992    0.000000   \n",
      "Gas Car, drove alone  1262.058190  26914.627463   533.425853  107.073464   \n",
      "Gas Car, with others  2437.828947  45015.959512   557.462681  162.329307   \n",
      "Pilot ebike           1705.574559   4069.273256  4870.244470   23.819570   \n",
      "Regular Bike           130.696623    129.675462  1210.862672    1.879016   \n",
      "Scooter share           17.604263    125.080958    17.826407    6.814621   \n",
      "Skate board              5.870961     13.402265     0.000000    0.000000   \n",
      "Taxi/Uber/Lyft          67.656482    621.263829     5.627856    3.016719   \n",
      "Train                   28.582915    168.018982    49.648131    0.000000   \n",
      "Walk                  2429.712733    785.394741   189.124594   13.325724   \n",
      "\n",
      "                        no_sensed       train   air_or_hsr      subway  \n",
      "Bikeshare                0.000000    0.000000     0.000000    0.000000  \n",
      "Bus                     89.254121   13.851809     0.000000    0.000000  \n",
      "E-car, drove alone       0.000000    0.000000     0.000000    0.000000  \n",
      "Free Shuttle             6.781510    4.542978     0.000000    0.000000  \n",
      "Gas Car, drove alone  1360.084829    0.000000    12.278501    0.000000  \n",
      "Gas Car, with others  2798.045205  106.677880  3679.049158    0.000000  \n",
      "Pilot ebike            430.634286   91.538160     0.170056    0.000000  \n",
      "Regular Bike            49.158939    0.000000     0.000000    0.000000  \n",
      "Scooter share            2.260719    0.000000     0.000000    0.000000  \n",
      "Skate board              3.044542    0.000000     0.000000    0.000000  \n",
      "Taxi/Uber/Lyft          52.591749    0.000000     0.000000    0.000000  \n",
      "Train                    0.000000  593.657879     0.000000  175.207894  \n",
      "Walk                   128.795647    0.000000     0.897537    0.000000  \n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "# make df for each half of data (use one half for distribution sampling, the other half for the estimating part)\n",
    "'''\n",
    "Split data and get two nice DFs\n",
    "'''\n",
    "df_EI = pd.read_csv(r'Public_Dashboard/auxiliary_files/energy_intensity.csv')\n",
    "energy_dict = cm_handling.get_energy_dict(df_EI, units='MWH')\n",
    "from confusion_matrix_handling import MODE_MAPPING_DICT\n",
    "\n",
    "# get result from running store_expanded_labeled_trips.ipynb\n",
    "%store -r expanded_labeled_trips \n",
    "expanded_labeled_trips = helper.get_primary_modes(expanded_labeled_trips[expanded_labeled_trips.section_modes.notna()], energy_dict, MODE_MAPPING_DICT)\n",
    "\n",
    "# split in half.\n",
    "from sklearn.utils import shuffle\n",
    "shuffled_trips = shuffle(expanded_labeled_trips)\n",
    "shuffled_trips = helper.drop_unwanted_trips(shuffled_trips, drop_not_a_trip=True)\n",
    "samplingCM = shuffled_trips.iloc[0:int(len(expanded_labeled_trips.index)/2)]\n",
    "testingCM = shuffled_trips.iloc[int(len(expanded_labeled_trips.index)/2):]\n",
    "\n",
    "split_by_os = True \n",
    "\n",
    "if split_by_os: # for distances\n",
    "    samplingCMiOS = samplingCM.loc[samplingCM['os'] == \"ios\"]\n",
    "    samplingCMandroid = samplingCM.loc[samplingCM['os'] == \"android\"]\n",
    "    testingCMiOS = testingCM.loc[testingCM['os'] == \"ios\"]\n",
    "    testingCMandroid = testingCM.loc[testingCM['os'] == \"android\"]\n",
    "\n",
    "    samplingCMiOS = pair_matching(samplingCMiOS, by_distances=True)\n",
    "    samplingCMandroid = pair_matching(samplingCMandroid, by_distances=True)\n",
    "    testingCMiOS = pair_matching(testingCMiOS, by_distances=True)\n",
    "    testingCMandroid = pair_matching(testingCMandroid, by_distances=True)\n",
    "\n",
    "    samplingCMiOS = map_labels(samplingCMiOS, MODE_MAPPING_DICT)\n",
    "    samplingCMandroid = map_labels(samplingCMandroid, MODE_MAPPING_DICT)\n",
    "    testingCMiOS = map_labels(testingCMiOS, MODE_MAPPING_DICT)\n",
    "    testingCMandroid = map_labels(testingCMandroid, MODE_MAPPING_DICT)\n",
    "\n",
    "    # convert from meters to miles\n",
    "    METERS_TO_MILES = 0.000621371 # 1 meter = 0.000621371 miles\n",
    "    samplingCMiOS = samplingCMiOS * METERS_TO_MILES\n",
    "    samplingCMandroid = samplingCMandroid * METERS_TO_MILES\n",
    "    testingCMiOS = testingCMiOS * METERS_TO_MILES\n",
    "    testingCMandroid = testingCMandroid * METERS_TO_MILES\n",
    "\n",
    "    testingCMiOS = fill_in([samplingCMiOS, samplingCMandroid, testingCMandroid], testingCMiOS)\n",
    "    testingCMandroid = fill_in([testingCMiOS], testingCMandroid)\n",
    "    samplingCMiOS = fill_in([testingCMiOS], samplingCMiOS)\n",
    "    samplingCMandroid = fill_in([testingCMiOS], samplingCMandroid)\n",
    "    \n",
    "    samplingCMiOS = samplingCMiOS.sort_index()\n",
    "    samplingCMandroid = samplingCMandroid.sort_index()\n",
    "    testingCMiOS = testingCMiOS.sort_index()\n",
    "    testingCMandroid = testingCMandroid.sort_index()\n",
    "\n",
    "    print(samplingCMiOS)\n",
    "    print(samplingCMandroid)\n",
    "    print(testingCMiOS)\n",
    "    print(testingCMandroid)\n",
    "\n",
    "elif not split_by_os: # for counts\n",
    "    # clean up\n",
    "    samplingCM = pair_matching(samplingCM, by_distances=False)\n",
    "    samplingCM = map_labels(samplingCM, MODE_MAPPING_DICT)\n",
    "    \n",
    "    testingCM = pair_matching(testingCM, by_distances=False)\n",
    "    testingCM = map_labels(testingCM, MODE_MAPPING_DICT)\n",
    "\n",
    "    samplingCM = fill_in([testingCM], samplingCM)\n",
    "    testingCM = fill_in([samplingCM], testingCM)\n",
    "\n",
    "    # also removing the no_sensed and air_or_hsr sensed labels since they don't have a corresp match in the standardized user labels\n",
    "    samplingCM = samplingCM.drop(labels=[\"air_or_hsr\", \"no_sensed\"], axis=1)\n",
    "    testingCM = testingCM.drop(labels=[\"air_or_hsr\", \"no_sensed\"], axis=1)\n",
    "\n",
    "    # making rows and columns the same order\n",
    "    testingCM = testingCM.loc[:, samplingCM.columns]\n",
    "    testingCM = testingCM.sort_index()\n",
    "    samplingCM = samplingCM.sort_index()\n",
    "\n",
    "    print(samplingCM)\n",
    "    print(testingCM)\n",
    "\n",
    "# this takes around 20s for the entire all_ceo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking        8516.187595\n",
      "car           80802.792877\n",
      "bicycling      7614.808000\n",
      "bus             374.239010\n",
      "no_sensed      4920.651546\n",
      "train           810.268706\n",
      "air_or_hsr     3692.395252\n",
      "subway          175.207894\n",
      "dtype: float64\n",
      "walking\n",
      "car\n",
      "bicycling\n",
      "bus\n",
      "no_sensed\n",
      "train\n",
      "air_or_hsr\n",
      "subway\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Bikeshare                  98.355018\n",
       " Bus                      4941.308390\n",
       " E-car, drove alone          7.711472\n",
       " Free Shuttle              129.211763\n",
       " Gas Car, drove alone    32785.371016\n",
       " Gas Car, with others    57563.947695\n",
       " Pilot ebike             12550.420043\n",
       " Regular Bike             1707.715750\n",
       " Scooter share             109.341396\n",
       " Skate board                35.536258\n",
       " Taxi/Uber/Lyft           1069.848613\n",
       " Train                    1733.393446\n",
       " Walk                     5511.605282\n",
       " dtype: float64,\n",
       " Bikeshare               7.780731e+03\n",
       " Bus                     3.977461e+05\n",
       " E-car, drove alone      1.905686e+03\n",
       " Free Shuttle            4.504362e+03\n",
       " Gas Car, drove alone    2.261677e+07\n",
       " Gas Car, with others    6.664904e+07\n",
       " Pilot ebike             1.744036e+06\n",
       " Regular Bike            7.870109e+04\n",
       " Scooter share           1.067583e+04\n",
       " Skate board             2.797220e+03\n",
       " Taxi/Uber/Lyft          3.840188e+04\n",
       " Train                   4.257177e+04\n",
       " Walk                    6.495045e+05\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_distances = testingCMandroid.sum(axis=0)\n",
    "find_distances(samplingCMandroid, predicted_distances, \"android\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Bikeshare                 65.894072\n",
       " Bus                      695.034071\n",
       " E-car, drove alone         4.637843\n",
       " Free Shuttle              53.544810\n",
       " Gas Car, drove alone    9357.277206\n",
       " Gas Car, with others    9677.533056\n",
       " Pilot ebike             9684.089111\n",
       " Regular Bike            1335.176330\n",
       " Scooter share             52.363406\n",
       " Skate board               24.997295\n",
       " Taxi/Uber/Lyft           204.438859\n",
       " Train                     84.497270\n",
       " Walk                    6840.516670\n",
       " dtype: float64,\n",
       " Bikeshare                 117.837569\n",
       " Bus                      1214.400858\n",
       " E-car, drove alone          7.817907\n",
       " Free Shuttle               96.335416\n",
       " Gas Car, drove alone    10976.570335\n",
       " Gas Car, with others    11257.610107\n",
       " Pilot ebike             10302.969667\n",
       " Regular Bike             2148.062369\n",
       " Scooter share              93.664400\n",
       " Skate board                43.598504\n",
       " Taxi/Uber/Lyft            377.138834\n",
       " Train                     121.362239\n",
       " Walk                     7239.889105\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_counts = testingCM.sum(axis=0)\n",
    "find_counts(samplingCM, predicted_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding distances for CanBikeCo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding distances\n",
    "'''\n",
    "iOS\n",
    "'''\n",
    "print(\"ios:\\n\")\n",
    "iosCMs = sampling(samplingCMiOS, n = 2000)\n",
    "iosProbs = actual_given_sensed_CM(iosCMs)\n",
    "iosEstimates = distance_estimate(testingCMiOS, iosProbs, \"ios\")\n",
    "\n",
    "'''\n",
    "android\n",
    "'''\n",
    "print(\"\\nandroid:\\n\")\n",
    "androidCMs = sampling(samplingCMandroid, n = 2000)\n",
    "androidProbs = actual_given_sensed_CM(androidCMs)\n",
    "androidEstimates = distance_estimate(testingCMandroid, androidProbs, \"android\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine results for ios and android distances just by adding\n",
    "total_distance_estimates = androidEstimates[0] + iosEstimates[0]\n",
    "total_variances = androidEstimates[1] + iosEstimates[1]\n",
    "\n",
    "actual = (testingCMandroid.add(testingCMiOS)).sum(axis = \"columns\").T\n",
    "sd_from_actual = (total_distance_estimates.subtract(actual)).div(np.sqrt(total_variances))\n",
    "\n",
    "print(pd.DataFrame({\"estimates\":total_distance_estimates, \"actual\":actual, \"sd from actual\": sd_from_actual}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine results for ios and android distances by a weighted sum\n",
    "total_android_distance = testingCMandroid.sum().sum()\n",
    "total_ios_distance = testingCMiOS.sum().sum()\n",
    "ios_weight = total_ios_distance / (total_ios_distance + total_android_distance)\n",
    "android_weight = 1 - ios_weight\n",
    "print(\"ios weight: \", ios_weight, \" android weight: \", android_weight, \"\\n\")\n",
    "\n",
    "total_weighted_estimates = (iosEstimates[0] * ios_weight).add(androidEstimates[0] * android_weight)\n",
    "total_weighted_variance = (iosEstimates[1] * ios_weight).add(androidEstimates[1] * android_weight)\n",
    "sd_from_actual = (total_weighted_estimates.subtract(actual)).div(np.sqrt(total_weighted_variance))\n",
    "\n",
    "print(pd.DataFrame({\"weighted estimate\":total_weighted_estimates, \"actual\":actual, \"sd from actual\": sd_from_actual}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding mode counts for CanBikeCo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding counts\n",
    "countSamples = sampling(samplingCM, 2000)\n",
    "countProbs = actual_given_sensed_CM(countSamples)\n",
    "print(countProbs)\n",
    "countEstimates = count_estimate(testingCM, countProbs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding distance estimates for canbikeco data after training on mobility net data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_probabilities(cm, prior_mode_probs):\n",
    "    p_predicted_given_actual = cm.div(cm.sum(axis=1), axis='rows')\n",
    "    likelihood_times_priors = p_predicted_given_actual.multiply(pd.Series(prior_mode_probs), axis='rows')\n",
    "    normalizing_constants = likelihood_times_priors.sum(axis='rows')\n",
    "    prob_actual_given_predicted_df = likelihood_times_priors.divide(normalizing_constants, axis='columns').copy()\n",
    "\n",
    "    return prob_actual_given_predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import confusion_matrix_handling as cm_handling\n",
    "from confusion_matrix_handling import MODE_MAPPING_DICT\n",
    "import helper_functions as helper\n",
    "df_EI = pd.read_csv(r'Public_Dashboard/auxiliary_files/energy_intensity.csv')\n",
    "energy_dict = cm_handling.get_energy_dict(df_EI, units='MWH')\n",
    "\n",
    "pd.set_option(\"display.precision\", 8)\n",
    "# csv files were generated in classification_analysis.ipynb from the MobilityNet repo\n",
    "training_android = pd.read_csv(\"myCoolCMs/mobilitynet_android.csv\", index_col=0).T\n",
    "training_ios = pd.read_csv(\"myCoolCMs/mobilitynet_ios.csv\", index_col=0).T\n",
    "\n",
    "# remove all no_gt and no_sensed\n",
    "training_android = cm_handling.drop_rows_and_columns(training_android, row_list=[\"NO_GT_START\", \"NO_GT_MIDDLE\", \"NO_GT_END\"], column_list=[\"NO_SENSED_START\", \"NO_SENSED_MIDDLE\", \"NO_SENSED_END\"])\n",
    "training_ios =cm_handling.drop_rows_and_columns(training_ios, row_list=[\"NO_GT_START\", \"NO_GT_MIDDLE\", \"NO_GT_END\"], column_list=[\"NO_SENSED_START\", \"NO_SENSED_MIDDLE\", \"NO_SENSED_END\", \"UNKNOWN\"])\n",
    "\n",
    "# get canbikeco data by OS\n",
    "%store -r expanded_labeled_trips \n",
    "expanded_labeled_trips = helper.get_primary_modes(expanded_labeled_trips[expanded_labeled_trips.section_modes.notna()], energy_dict, MODE_MAPPING_DICT)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "shuffled_trips = shuffle(expanded_labeled_trips)\n",
    "shuffled_trips = helper.drop_unwanted_trips(shuffled_trips, drop_not_a_trip=True)\n",
    "\n",
    "testing_android = shuffled_trips.loc[shuffled_trips['os'] == \"android\"]\n",
    "testing_ios = shuffled_trips.loc[shuffled_trips['os'] == \"ios\"]\n",
    "\n",
    "# make canbikeco trips into CMs\n",
    "testing_android = pair_matching(testing_android, by_distances=True)\n",
    "testing_ios = pair_matching(testing_ios, by_distances=True)\n",
    "\n",
    "from confusion_matrix_handling import MODE_MAPPING_DICT\n",
    "\n",
    "testing_android = map_labels(testing_android, MODE_MAPPING_DICT)\n",
    "testing_ios = map_labels(testing_ios, MODE_MAPPING_DICT)\n",
    "\n",
    "all_ceo_to_MN_label_map = {\n",
    "    \"Regular Bike\": \"Bicycling\",\n",
    "    \"Walk\":\"Walking\"\n",
    "}\n",
    "\n",
    "testing_android = map_labels(testing_android, all_ceo_to_MN_label_map, remove_unknown=False)\n",
    "testing_ios = map_labels(testing_ios, all_ceo_to_MN_label_map, remove_unknown=False)\n",
    "    \n",
    "print(testing_android)\n",
    "print(\"******************\")\n",
    "print(training_android)\n",
    "\n",
    "# MN has labels in all caps....\n",
    "testing_android.columns = map(lambda x: str(x).upper(), testing_android.columns)\n",
    "testing_android.index = map(lambda x: str(x).upper(), testing_android.index)\n",
    "testing_ios.columns = map(lambda x: str(x).upper(), testing_ios.columns)\n",
    "testing_ios.index = map(lambda x: str(x).upper(), testing_ios.index)\n",
    "\n",
    "testing_android = cm_handling.drop_rows_and_columns(testing_android, row_list=[], column_list=[\"NO_SENSED\"])\n",
    "testing_ios = cm_handling.drop_rows_and_columns(testing_ios, row_list=[], column_list=[\"NO_SENSED\"])\n",
    "\n",
    "# make sure all CMs have the same columns and rows\n",
    "training_android = fill_in([training_ios, testing_android, testing_ios], training_android)\n",
    "training_ios = fill_in([training_android], training_ios)\n",
    "testing_android = fill_in([training_android], testing_android)\n",
    "testing_ios = fill_in([training_android], testing_ios)\n",
    "\n",
    "\n",
    "# put all CM indexes into the same order\n",
    "training_ios = training_ios.sort_index()\n",
    "training_android = training_android.sort_index()\n",
    "testing_ios = testing_ios.sort_index()\n",
    "testing_android = testing_android.sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Do the thing\n",
    "'''\n",
    "# find priors of testing set\n",
    "total_testing = testing_android.add(testing_ios)\n",
    "priors = total_testing.sum(axis=1) / total_testing.sum().sum()\n",
    "print(priors)\n",
    "print(\"Android results: \\n\")\n",
    "android_samples = sampling(training_android, 2000)\n",
    "android_probabilities = [] \n",
    "for cm in android_samples:\n",
    "    android_probabilities.append(update_probabilities(cm, priors))\n",
    "android_distance_estimates = distance_estimate(testing_android, android_probabilities, 'android')\n",
    "\n",
    "print(\"\\niOS results: \\n\")\n",
    "ios_samples = sampling(training_ios, 2000)\n",
    "ios_probabilities = []\n",
    "for cm in ios_samples:\n",
    "    ios_probabilities.append(update_probabilities(cm, priors))\n",
    "ios_distance_estimates = distance_estimate(testing_ios, ios_probabilities, 'ios')\n",
    "\n",
    "# combine results for ios and android distances by addition\n",
    "total_distance = android_distance_estimates[0] + ios_distance_estimates[0]\n",
    "total_variances = android_distance_estimates[1] + ios_distance_estimates[1]\n",
    "\n",
    "actual = (testing_android.add(testing_ios)).sum(axis = \"columns\").T\n",
    "sd_from_actual = (total_distance.subtract(actual)).div(np.sqrt(total_variances))\n",
    "\n",
    "print(\"\\nCombined results: \\n\", pd.DataFrame({\"estimates\":total_distance, \"actual\":actual, \"sd from actual\": sd_from_actual}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell entries / row sums of MN, CBCO for michael\n",
    "\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3], \"b\":[4, 5, 6]})\n",
    "df.div(df.sum(axis=1), axis=0)\n",
    "\n",
    "# MN distances data\n",
    "android = pd.read_csv(\"myCoolCMs/mobilitynet_android.csv\", index_col=0).T\n",
    "ios = pd.read_csv(\"myCoolCMs/mobilitynet_ios.csv\", index_col=0).T\n",
    "mn_total = android.add(ios)\n",
    "mn_total = cm_handling.drop_rows_and_columns(mn_total, row_list=[], column_list=[\"AIR_OR_HSR\", \"LIGHT_RAIL\", \"UNKNOWN\"])\n",
    "# mn_total = mn_total.div(mn_total.sum(axis=1), axis=0)\n",
    "# mn_total.to_csv(\"MobilityNet_distances_predicted_given_gt.csv\")\n",
    "\n",
    "# all_ceo count data\n",
    "# cbco_counts = samplingCM.add(testingCM)\n",
    "# cbco_counts = cbco_counts.div(cbco_counts.sum(axis=1), axis=0)\n",
    "# cbco_counts.to_csv(\"All_CEO_counts_predicted_given_gt.csv\")\n",
    "\n",
    "# all_ceo distance data\n",
    "cbco_distances = samplingCMandroid.add(testingCMandroid).add(samplingCMiOS).add(testingCMiOS)\n",
    "# cbco_distances = cbco_distances.div(cbco_distances.sum(axis=1), axis=0)\n",
    "# print(cbco_distances)\n",
    "# cbco_distances.to_csv(\"All_CEO_distances_predicted_given_gt.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emission-private-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
