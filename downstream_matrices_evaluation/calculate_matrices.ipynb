{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Author: Wen Zhang\n",
                "\n",
                "Timeline: \n",
                "1. 06.27.2023 - 07.10.2023 Coding EC\n",
                "2. 07.10.2023 - 07.11.2023 Coding CE\n",
                "3. 07.12.2023 - 07.14.2023 Coding Mode count\n",
                "4. 07.17.2023 - 07.20.2023 Coding Mode shared by distance\n",
                "5. 07.24.2023   Coding Mode count/distance new method \n",
                "5. 07.25.2023 - 07.28.2023 Analysing variance of Mode distance\n",
                "\n",
                "Sections of this notebook:\n",
                "1. Imports: inputing the classes from other files\n",
                "2. Loading data: loading the data from docker\n",
                "3. Running model and getting the testing and validation dataset and their predictions\n",
                "4. Establishing each user's confusion matrix\n",
                "5. Calculating mean and variance of energy intensity, carbon intensity, energy consumption and carbon emission\n",
                "6. Plotting the results of energy consumption and carbon emission\n",
                "7. Mode count and Mode shared by distance\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from uuid import UUID\n",
                "import os\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import sys \n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(),\"../..\")) + '/e-mission-server')\n",
                "import emission.storage.timeseries.abstract_timeseries as esta\n",
                "import emission.storage.decorations.trip_queries as esdtq\n",
                "\n",
                "sys.path.append(os.path.abspath(os.path.dirname(os.getcwd())) + \"/TRB_label_assist\")\n",
                "import performance_eval\n",
                "import models\n",
                "sys.path.append(os.path.abspath(os.path.dirname(os.getcwd())) + '/Error_bars/Public_Dashboard/auxiliary_files')\n",
                "\n",
                "\n",
                "sys.path.append(os.path.abspath(os.path.dirname(os.getcwd())) + '/Error_bars')\n",
                "import confusion_matrix_handling as cm_handling\n",
                "import get_EC,helper_functions\n",
                "import uuid\n",
                "import math\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set the display options to show more rows and columns\n",
                "pd.set_option('display.max_rows', 100)\n",
                "pd.set_option('display.max_columns', 100)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_users = esta.TimeSeries.get_uuid_list()\n",
                "confirmed_trip_df_map = {}\n",
                "labeled_trip_df_map = {}\n",
                "expanded_labeled_trip_df_map = {}\n",
                "expanded_all_trip_df_map = {}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# loading the data from the docker, cost about 5-6 minutes\n",
                "for u in all_users:\n",
                "    ts = esta.TimeSeries.get_time_series(u)\n",
                "    ct_df = ts.get_data_df(\"analysis/confirmed_trip\")\n",
                "\n",
                "    confirmed_trip_df_map[u] = ct_df\n",
                "    labeled_trip_df_map[u] = esdtq.filter_labeled_trips(ct_df)\n",
                "    expanded_labeled_trip_df_map[u] = esdtq.expand_userinputs(\n",
                "        labeled_trip_df_map[u])\n",
                "    expanded_all_trip_df_map[u] = esdtq.expand_userinputs(\n",
                "        confirmed_trip_df_map[u])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Length of the total dataset\n",
                "total_len = 0\n",
                "for ele in range(len(all_users)):\n",
                "    total_len += len(expanded_labeled_trip_df_map[all_users[ele]])\n",
                "total_len"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "check how many labeled/unlabeled trips there are:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "n_trips_df = pd.DataFrame(\n",
                "    [[u, len(confirmed_trip_df_map[u]),\n",
                "      len(labeled_trip_df_map[u])] for u in all_users],\n",
                "    columns=[\"user_id\", \"all_trips\", \"labeled_trips\"])\n",
                "\n",
                "all_trips = n_trips_df.all_trips.sum()\n",
                "labeled_trips = n_trips_df.labeled_trips.sum()\n",
                "unlabeled_trips = all_trips - labeled_trips\n",
                "n_users = len(n_trips_df)\n",
                "\n",
                "print('{} ({:.2f}%) unlabeled, {} ({:.2f}%) labeled, {} total trips'.format(\n",
                "    unlabeled_trips, unlabeled_trips / all_trips, labeled_trips,\n",
                "    labeled_trips / all_trips, all_trips))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Running model and get the testing and validation dataset and their predictions"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The following cell will load the cross-validation results for the listed models. \n",
                "In the initial TRB_label_assist/performance_eval.py, there are list of models. However I commented the most of the models except for the\n",
                "\"random forests (coordinates)\", becasue it is the best model we found and now we only need to evaluate the downstream metrics based on \"the best model\".\n",
                "\n",
                "'cv_results' contains the result of \"random forests (coordinates)\", which are testing dataset and the validation dataset.\n",
                "Here the testing dataset contains 80% of the total data and validation dataset contains 20% of the total data.\n",
                "Testing dataset contains 80% of the total data. This becasue we use k-fold cross validation for the traning and testing dataset.\n",
                "For example, if the total dataset contains 100 trips, we separate the dataset to temp_dataset 80% and validation dataset 20%. For the temp_dataset we seperate the data to 4 part, each part has 20 trips. We can label the each part as 1,2,3,4. \n",
                "1. First fold, We use part 1 as testing dataset and part 2,3,4 as the training dataset. \n",
                "2. Second fold, We use part 2 as testing dataset and part 1,3,4 as the training dataset.  \n",
                "3. Third fold, We use part 3 as testing dataset and part 1,2,4 as the training dataset.  \n",
                "4. Forth fold, We use part 4 as testing dataset and part 1,2,3 as the training dataset.  \n",
                "After 4-fold cross validation, we have the prediction of temp_dataset, then we aim this 80% of the total dataset as the test dataset.\n",
                "\n",
                "Note: If the cross-validation results for the model have already been generated, it will attempt to load it from the csv file to avoid the time-consuming process of re-running it. Otherwise, it will run the cross-validation from scratch. (This feature can be toggled with the override_prior_runs parameter - if True, it will ignore existing csv's and re-run from scratch.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Doing 4-fold cross validation for the temp dataset, k=4. \n",
                "# Also, getting the testing and validation dataset and their predictions\n",
                "\n",
                "model_names = list(performance_eval.PREDICTORS.keys())\n",
                "cv_results = performance_eval.cv_for_all_algs(\n",
                "    uuid_list=all_users,\n",
                "    expanded_trip_df_map=expanded_labeled_trip_df_map,\n",
                "    model_names=model_names,\n",
                "    override_prior_runs=False,\n",
                "    k=4, # 4-fold \n",
                "    raise_errors=False,\n",
                "    random_state=42,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "# cv_results contains test_trips and validation_trips\n",
                "RFc_df = pd.DataFrame(cv_results['random forests (coordinates)'])\n",
                "\n",
                "# get distance in 'miles'\n",
                "METERS_TO_MILES = 0.000621371 # 1 meter = 0.000621371 miles\n",
                "RFc_df['distance_miles'] = RFc_df.distance*METERS_TO_MILES\n",
                "\n",
                "# get distance in 'km'\n",
                "RFc_df['distance_km'] = RFc_df.distance/1000\n",
                "\n",
                "# get validation_trips\n",
                "validation_trips = RFc_df[RFc_df['dataset'] == 'validation_dataset']\n",
                "\n",
                "# get test_trips\n",
                "test_trips = RFc_df[RFc_df['dataset'] != 'validation_dataset']"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### The below three cells are a try for the conbimed confusion matrixes (can ignore)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_user_confusion_matrices_dic(mode_results_df):\n",
                "    attribute = 'distance'\n",
                "        \n",
                "    user_confusion_matrices_dic0 = {}\n",
                "\n",
                "    grouped_user = mode_results_df.groupby('user_id')\n",
                "    for user_id, group_df in grouped_user:\n",
                "        predicted_values = group_df['predicted_value']\n",
                "        true_values = group_df['true_value']\n",
                "        if attribute == 'distance':\n",
                "            sample_weight = group_df['distance_km']\n",
                "            confusion_matrix = pd.crosstab(true_values, predicted_values, sample_weight, aggfunc='sum') # weight: trip distance\n",
                "        elif attribute == 'duration':\n",
                "            sample_weight = group_df['duration']\n",
                "            confusion_matrix = pd.crosstab(true_values, predicted_values, sample_weight, aggfunc='sum') # weight: trip duration\n",
                "        else: # attribute: 'tripCount'\n",
                "            confusion_matrix = pd.crosstab(true_values, predicted_values) # weight: trip count\n",
                "\n",
                "        confusion_matrix[confusion_matrix.isnull()] = 0\n",
                "\n",
                "        user_confusion_matrices_dic0[user_id] = confusion_matrix\n",
                "        \n",
                "    return user_confusion_matrices_dic0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def is_square_matrix(matrix):\n",
                "    # Get the number of rows and columns of the matrix\n",
                "    num_rows, num_cols = matrix.shape\n",
                "    \n",
                "    # Check if the matrix is square (number of rows equals number of columns)\n",
                "    return num_rows == num_cols"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create multiply confusion matrixes base on the k-fold results.\n",
                "# Create conbimed confusion matrixes\n",
                "# Get the nn_test_trips which contains the test trips of user who has N*N summed_matrix.\n",
                "test_trip_user_list = list(test_trips['user_id'].unique())\n",
                "not_square_matrix_num = 0\n",
                "square_matrix_num = 0\n",
                "nn_test_trips = pd.DataFrame()\n",
                "valid_sum_count = 0\n",
                "for user in test_trip_user_list:\n",
                "    user_trips = test_trips[test_trips['user_id']==user]\n",
                "    user_validation_trips = validation_trips[validation_trips['user_id']==user]\n",
                "    user_validation_trips_pred_mode = list(user_validation_trips.mode_pred.unique())\n",
                "\n",
                "    grouped = user_trips.groupby('fold_number_list')\n",
                "    grouped_dataframes = {}\n",
                "    for group_name, group_data in grouped:\n",
                "        grouped_dataframes[group_name] = group_data\n",
                "\n",
                "    user_confusion_matrices_dic0 = {}\n",
                "    user_confusion_matrices_dic1 = {}\n",
                "    user_confusion_matrices_dic2 = {}\n",
                "    user_confusion_matrices_dic3 = {}\n",
                "\n",
                "    for gd_index in range(len(grouped_dataframes)): # len(grouped_dataframes) = 4\n",
                "        mode_results = {}\n",
                "        # get results\n",
                "        results = performance_eval.get_clf_metrics(grouped_dataframes[gd_index],\n",
                "                                    \"mode\",\n",
                "                                    # weight='distance',\n",
                "                                    keep_nopred=True,\n",
                "                                    ignore_custom=False)\n",
                "\n",
                "        mode_results['predicted_value'] = results['label_pred']\n",
                "        mode_results['true_value'] = results['label_true']\n",
                "        mode_results['user_id'] = results['user_id']\n",
                "        mode_results['duration'] = results['duration']\n",
                "        mode_results['distance_meter'] = results['trip_dists']\n",
                "        mode_results['distance_km'] = results['trip_dists']/1000\n",
                "\n",
                "        METERS_TO_MILES = 0.000621371 # 1 meter = 0.000621371 miles\n",
                "        mode_results['distance_miles'] = results['trip_dists']*METERS_TO_MILES\n",
                "        mode_results_df = pd.DataFrame(mode_results)\n",
                "        \n",
                "        if gd_index==0:\n",
                "            user_confusion_matrices_dic0 = get_user_confusion_matrices_dic(mode_results_df)\n",
                "\n",
                "        if gd_index==1:\n",
                "            user_confusion_matrices_dic1 = get_user_confusion_matrices_dic(mode_results_df)\n",
                "\n",
                "        if gd_index==2:\n",
                "            user_confusion_matrices_dic2=get_user_confusion_matrices_dic(mode_results_df)\n",
                "\n",
                "        if gd_index==3:\n",
                "            user_confusion_matrices_dic3=get_user_confusion_matrices_dic(mode_results_df)\n",
                "\n",
                "    CM0 = user_confusion_matrices_dic0.get(user)\n",
                "    CM1 = user_confusion_matrices_dic1.get(user)\n",
                "\n",
                "    CM2 = user_confusion_matrices_dic2.get(user)\n",
                "    CM3 = user_confusion_matrices_dic3.get(user)\n",
                "\n",
                "    summed_matrix = pd.DataFrame()\n",
                "    mean_matrix = pd.DataFrame()\n",
                "    median_matrix = pd.DataFrame()\n",
                "\n",
                "    all_matrices = [CM0, CM1, CM2, CM3]\n",
                "\n",
                "    for matrix in all_matrices:\n",
                "        summed_matrix = summed_matrix.add(matrix, fill_value=0)\n",
                "\n",
                "    mean_matrix = summed_matrix / len(all_matrices)\n",
                "\n",
                "    combined_matrix = pd.concat(all_matrices)\n",
                "    median_matrix = combined_matrix.groupby(level=0).median()\n",
                "\n",
                "\n",
                "    mean_matrix = mean_matrix.fillna(0)\n",
                "    median_matrix = median_matrix.fillna(0)\n",
                "    summed_matrix = summed_matrix.fillna(0)\n",
                "    CM0 =CM0.fillna(0)\n",
                "    CM1 =CM1.fillna(0)\n",
                "    CM2 =CM2.fillna(0)\n",
                "    CM3 =CM3.fillna(0)\n",
                "\n",
                "    if is_square_matrix(summed_matrix) and list(summed_matrix.index)==list(summed_matrix.columns) and set(summed_matrix.index).issubset(set(user_validation_trips_pred_mode)):\n",
                "        square_matrix_num +=1\n",
                "        nn_test_trips = nn_test_trips.append(user_trips)\n",
                "        if len(set(summed_matrix.index))<len(set(user_validation_trips_pred_mode)):\n",
                "            valid_sum_count += 1\n",
                "        \n",
                "    else:\n",
                "        not_square_matrix_num +=1\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the user_id list in the test list\n",
                "# if we want to use nn_test_trips, we need to change the below code 'test_trips' to 'nn_test_trips'.\n",
                "test_trips_user_list = list(test_trips.user_id.unique())"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### clean validation_trips and add device information"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "# clean validation_trips \n",
                "validation_trips = validation_trips.reset_index(drop=True)\n",
                "validation_trips = validation_trips.rename(columns={\"mode_initial\": \"mode_confirm\"})\n",
                "\n",
                "# add device information for validation_trips. device can be 'ios' or 'android'\n",
                "validation_trips['os'] = ['ios' if x == 'DwellSegmentationDistFilter' else 'android' for x in validation_trips['source']]\n",
                "validation_trips['user_id'] = validation_trips['user_id'].astype(str)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# if we want to check partly users in the test_trips, we need to filter the validation_trips for those users also.\n",
                "validation_trips = validation_trips[validation_trips['user_id'].isin(test_trips_user_list)]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Establishing each user's confusion matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Regularize the form of testing dataset\n",
                "mode_results = {}\n",
                "\n",
                "for model_name in cv_results.keys(): # only one model's results will be in the cv_results.keys(), because we are only using the best model which is random forests (coordinates)\n",
                "    print(f'now geting: {model_name}')\n",
                "    # get 'label_pred', 'label_true', 'user_id','duration', 'trip_dists' of test_trips\n",
                "    results = performance_eval.get_clf_metrics(test_trips,\n",
                "                                \"mode\",\n",
                "                                keep_nopred=True,\n",
                "                                ignore_custom=False)\n",
                "\n",
                "    mode_results['predicted_value'] = results['label_pred']\n",
                "    mode_results['true_value'] = results['label_true']\n",
                "    mode_results['user_id'] = results['user_id']\n",
                "    mode_results['duration'] = results['duration']\n",
                "    mode_results['distance_meter'] = results['trip_dists']\n",
                "\n",
                "    # Get distance in miles\n",
                "    METERS_TO_MILES = 0.000621371 # 1 meter = 0.000621371 miles\n",
                "    mode_results['distance_miles'] = results['trip_dists']*METERS_TO_MILES\n",
                "\n",
                "    # Get distance in km\n",
                "    mode_results['distance_km'] = results['trip_dists']/1000\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [],
            "source": [
                "# save the prediction and the ground truth of the testing dataset\n",
                "pd.DataFrame(mode_results).to_csv(\"CSVs/compare_true_pred_mode.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# read the prediction and the ground truth of the testing dataset\n",
                "mode_df = pd.DataFrame(pd.read_csv(\"CSVs/compare_true_pred_mode.csv\"))\n",
                "mode_df = mode_df.drop(['Unnamed: 0'], axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build a confusion matrix for each user based on the prediction and the ground truth of 'attribute'\n",
                "# 'attribute' can be 'distance', 'duration' and 'tripCount'. The value of 'attribute' will also show in the plot and the file name (later we will save some files).\n",
                "attribute = 'distance'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get each user's confusion matrices and store in a dictionary\n",
                "user_confusion_matrices_dic = {}\n",
                "\n",
                "grouped_user = mode_df.groupby('user_id')\n",
                "for user_id, group_df in grouped_user:\n",
                "    predicted_values = group_df['predicted_value']\n",
                "    true_values = group_df['true_value']\n",
                "    if attribute == 'distance':\n",
                "        sample_weight = group_df['distance_km']\n",
                "        confusion_matrix = pd.crosstab(true_values, predicted_values, sample_weight, aggfunc='sum') # weight: trip distance\n",
                "    elif attribute == 'duration':\n",
                "        sample_weight = group_df['duration']\n",
                "        confusion_matrix = pd.crosstab(true_values, predicted_values, sample_weight, aggfunc='sum') # weight: trip duration\n",
                "    else: # attribute: 'tripCount'\n",
                "        confusion_matrix = pd.crosstab(true_values, predicted_values) # weight: trip count\n",
                "\n",
                "    confusion_matrix[confusion_matrix.isnull()] = 0\n",
                "    user_confusion_matrices_dic[user_id] = confusion_matrix\n",
                "    \n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Calculating mean and variance of energy intensity, carbon intensity, energy consumption and carbon emission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get mean and variance of different devices(unit_distance_MCS.csv)\n",
                "unit_dist_MCS_df = pd.read_csv(os.path.abspath(os.path.dirname(os.getcwd())) + '/Error_bars/unit_distance_MCS.csv').set_index(\"moment\")\n",
                "\n",
                "# get energy intensity file (energy_intensity.csv)\n",
                "df_EI = pd.read_csv(os.path.abspath(os.path.dirname(os.getcwd())) + '/Error_bars/Public_Dashboard/auxiliary_files/energy_intensity.csv') # r stands for raw string, only matters if the path is on Windows"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "unit_distance_MCS.csv:\n",
                "\n",
                "| moment | android   | ios   |\n",
                "|------|------|------|\n",
                "| mean | ...| ...|\n",
                "| var  | ...| ... |"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "energy_intensity.csv\n",
                "\n",
                "|mode|fuel|(kWH)/trip|EI(kWH/PMT)|energy_intensity_factor|energy_intensity_units|CO2_factor|CO2_factor_units|\n",
                "|------|------|------|------|------|------|------|------|\n",
                "|\"Gas Car, drove alone\"|gasoline|0||...|BTU/PMT|...|lb_CO2/MMBTU|\n",
                "|...|...|...|...|...|...|...|...|"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_elt_with_errors(valid_trips_user, user_moments_df, unit_dist_MCS_df, elt_with_errors_all, intensity_dict):\n",
                "    '''\n",
                "        Inputs:\n",
                "            valid_trips_user: one user's validation dataset\n",
                "            user_moments_df: one user's mean and variance of the energy intensity or carbon intensity\n",
                "            unit_dist_MCS_df: mean and variance of the devices error\n",
                "            elt_with_errors_all: an empty DataFrame\n",
                "            intensity_dict: dictionary by mode of energy intensities or by mode of carbon intensities\n",
                "\n",
                "        Outputs:\n",
                "            1. dictionary contains:\n",
                "            (1) dif_expected_user_laberd_mean: distance from expected mean to user_laberd mean\n",
                "            (2) expected_mean: expected mean\n",
                "            (3) user_labeled_mean: user labeled mean\n",
                "            (4) all_mode_expected_SD_EC: standard deviation of this user's all trips. calculate the variance of each mode first, then combine the variance of all the modes.\n",
                "            2. elt_with_errors_all: each trip's information, e.g.expected value, user_labeled value, variance of expected value, variance of user_labeled value and so on.\n",
                "    '''\n",
                "\n",
                "    expected = []\n",
                "    user_labeled = []\n",
                "    confusion_based_variance = []\n",
                "    user_based_variance = []\n",
                "    expected_error_list = []\n",
                "\n",
                "    EI_length_covariance = 0\n",
                "    \n",
                "    # iterate each trip of this user\n",
                "    for _,ct in valid_trips_user.iterrows():\n",
                "        # Calculate expected energy consumption\n",
                "        ct[\"section_modes\"] =  [ct[\"mode_true\"]]\n",
                "\n",
                "        # according to the device used for this trip, assign the mean and variane of device to this trip.\n",
                "        ct[\"section_distances\"] =  [ct[\"distance\"]]\n",
                "        if ct['os']=='ios':\n",
                "            ios_EI_moments = user_moments_df\n",
                "            android_EI_moments = pd.DataFrame()\n",
                "        elif ct['os']=='android':\n",
                "            android_EI_moments = user_moments_df\n",
                "            ios_EI_moments = pd.DataFrame()\n",
                "\n",
                "        # calculate the mean and the variance of the energy consumption and carbon emission. \n",
                "        # get_EC.get_expected_EC_for_one_trip() was created for EC, but it also works for CE. Same logic, the only difference is that user_moments_df comes from EI or CI.\n",
                "        trip_expected, trip_confusion_based_variance = get_EC.get_expected_EC_for_one_trip(ct,unit_dist_MCS_df,android_EI_moments,ios_EI_moments, EI_length_covariance)\n",
                "\n",
                "        # Calculate the mean and the variance of the user labeled energy consumption\n",
                "        trip_user_labeled, trip_user_based_variance = get_EC.get_user_labeled_EC_for_one_trip(ct,unit_dist_MCS_df,intensity_dict)\n",
                "\n",
                "        expected.append(trip_expected)\n",
                "        user_labeled.append(trip_user_labeled)\n",
                "\n",
                "        confusion_based_variance.append(trip_confusion_based_variance)\n",
                "        user_based_variance.append(trip_user_based_variance)\n",
                "            \n",
                "        expected_error = trip_expected - trip_user_labeled\n",
                "\n",
                "        expected_error_list.append(expected_error)\n",
                "        # if (abs(expected_error) > 100):\n",
                "        #     print(f\"Large EC error: EC user labeled, EC expected: {trip_user_labeled:.2f}, {trip_expected:.2f}\")\n",
                "        #     print(f\"\\tTrip info: mode_confirm,sensed,distance (mi): {ct['mode_confirm'],ct['section_modes']},{ct['distance']*METERS_TO_MILES:.2f}\")\n",
                "\n",
                "    total_expected = sum(expected)\n",
                "    total_user_labeled = sum(user_labeled)\n",
                "\n",
                "    percent_error_expected = helper_functions.relative_error(total_expected,total_user_labeled)*100\n",
                "\n",
                "    # Append the values to expanded_labeled_trips\n",
                "    elt_with_errors = valid_trips_user.copy()  # elt: expanded labeled trips\n",
                "    elt_with_errors['error_for_confusion'] = expected_error_list\n",
                "    elt_with_errors['expected'] = expected\n",
                "    elt_with_errors['user_labeled'] = user_labeled\n",
                "\n",
                "    # Append variances\n",
                "    elt_with_errors['confusion_var'] = confusion_based_variance\n",
                "    elt_with_errors['user_var'] = user_based_variance\n",
                "    elt_with_errors['confusion_sd'] = np.sqrt(np.array(confusion_based_variance))\n",
                "    elt_with_errors['user_sd'] = np.sqrt(np.array(user_based_variance))\n",
                "\n",
                "    # expected mean, user_laberd mean   # e.g. (0.22, 0.4)\n",
                "    expected_mean = elt_with_errors.expected.sum()\n",
                "    user_labeled_mean = elt_with_errors.user_labeled.sum()\n",
                "    \n",
                "    # distance from expected mean to user_laberd mean\n",
                "    dif_expected_user_laberd_mean =  expected_mean - user_labeled_mean # e.g. 0.18\n",
                "   \n",
                "    os_EI_moments_map = {'ios': user_moments_df, 'android': user_moments_df} # assume the trips in the train and test dataset use same OS \n",
                "    valid_trips_user['primary_mode'] = valid_trips_user['mode_true']\n",
                "    all_mode_expected_variance_EC = get_EC.compute_aggregate_variance_by_primary_mode(valid_trips_user, os_EI_moments_map, unit_dist_MCS_df)\n",
                "\n",
                "    elt_with_errors_all = elt_with_errors_all.append(elt_with_errors)\n",
                "\n",
                "    # standard deviation of expected   # e.g. 0.17\n",
                "\n",
                "    return {'dif_expected_user_laberd_mean': dif_expected_user_laberd_mean, 'expected_mean': expected_mean, 'user_labeled_mean': user_labeled_mean,\n",
                "            'all_mode_expected_SD_EC': np.sqrt(all_mode_expected_variance_EC)}, elt_with_errors_all\n",
                "\n",
                "    # # standard deviation of user_laberd   # e.g. 0.104\n",
                "    # print(np.sqrt(elt_with_errors.user_var.sum()))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Calculate Expected matrixes and Uncertenties"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [],
            "source": [
                "def user_elt_with_errors_method(intensity_dict, user_confusion_matrices_dic, unit_dist_MCS_df, validation_trips):\n",
                "\n",
                "    ''' Inputs:\n",
                "            intensity_dict: energy/carbon intensity\n",
                "            user_confusion_matrices_dic: dictionary conatains all users' confusion matrix \n",
                "            unit_dist_MCS_df: mean and variance of different devices (ios and android)\n",
                "            validation_trips: validation dataset \n",
                "\n",
                "        Returns:\n",
                "            user_elt_with_errors and elt_with_errors_all\n",
                "            user_elt_with_errors is a dictionary. The key is the user's id, value is a dictionary which contains:\n",
                "                (1) dif_expected_user_laberd_mean: distance from expected mean to user_laberd mean\n",
                "                (2) expected_mean: expected mean\n",
                "                (3) user_labeled_mean: user labeled mean\n",
                "                (4) all_mode_expected_SD_EC: standard deviation of this user's all trips. calculate the variance of each mode first, then combine the variance of all the modes.\n",
                "                \n",
                "            elt_with_errors_all is a DataFrame. It contains all the user's all the trips. Row is a trip, it contains trip information, e.g.expected value, user_labeled value, variance of expected value, variance of user_labeled value and so on.        \n",
                "    '''\n",
                "\n",
                "    user_elt_with_errors = {}\n",
                "    elt_with_errors_all = pd.DataFrame()\n",
                "    \n",
                "    # iterate each user's confusion matrix.\n",
                "    for user_id, confusion_matrix in user_confusion_matrices_dic.items():\n",
                "        # confusion_matrix_df is each user's confusion matrix, predicted_value VS true_value\n",
                "        confusion_matrix_df = pd.DataFrame(confusion_matrix) \n",
                "\n",
                "        # get mean and variance of expected energy/carbon intensity base on the confusion matrix and ground truth energy/carbon intensity\n",
                "        user_EI_moments_df = cm_handling.get_conditional_EI_expectation_and_variance(confusion_matrix_df, intensity_dict) \n",
                "\n",
                "        # get user's validation dataset.\n",
                "        valid_trips_user = validation_trips[validation_trips['user_id']== user_id]\n",
                "        \n",
                "        user_elt_with_errors[user_id], elt_with_errors_all = get_elt_with_errors(valid_trips_user, user_EI_moments_df, unit_dist_MCS_df, elt_with_errors_all, intensity_dict)\n",
                "        \n",
                "    return user_elt_with_errors, elt_with_errors_all"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculating carbon intensity\n",
                "def get_carbon_dict(energy_carbon_dataframe):\n",
                "    '''\n",
                "    energy_carbon_dataframe: dataframe based on energy_intensity.csv\n",
                "    units: lb/PMT\n",
                "\n",
                "    Returns a dictionary by mode of carbon intensity in lb/PMT\n",
                "    '''\n",
                "    carbon_dict = {}\n",
                "    for _,row in energy_carbon_dataframe.iterrows():\n",
                "        # if BTU/PMT -> MMBTU/PMT -> lb_CO_2/PMT (diesel or gas)\n",
                "        # else kWH/PMT -> MWH/PMT -> lb_CO_2/PMT (electric)\n",
                "        if row[\"fuel\"] not in [\"electric\",\"human_powered\"] :\n",
                "            carbon_intensity_lb = row[\"energy_intensity_factor\"] * 0.000001 * row[\"CO2_factor\"] #btu\n",
                "        else:\n",
                "            carbon_intensity_lb = row[\"energy_intensity_factor\"] * 0.001 * row[\"CO2_factor\"]\n",
                "        carbon_dict[row['mode']] = carbon_intensity_lb\n",
                "\n",
                "    # Add 'no_gt'\n",
                "    carbon_dict['no_gt'] = 0\n",
                "    return carbon_dict\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setting matrix\n",
                "# 'matrix' can be 'energy consumption' or 'carbon emission'\n",
                "matrix = 'carbon emission'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "intensity_dict = {}\n",
                "\n",
                "if matrix == 'carbon emission':\n",
                "    unit = 'lb/PMT'\n",
                "    intensity_dict = get_carbon_dict(df_EI)\n",
                "elif matrix == 'energy consumption':\n",
                "    unit = 'MWH'\n",
                "    intensity_dict = cm_handling.get_energy_dict(df_EI, units='MWH')\n",
                "\n",
                "user_elt_with_errors, elt_with_errors_all = user_elt_with_errors_method(intensity_dict, user_confusion_matrices_dic, unit_dist_MCS_df, validation_trips)\n",
                "user_elt_with_errors_df = pd.DataFrame(data = user_elt_with_errors)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plotting the results of energy consumption and carbon emission"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Difference between Expected and User labeled matrix VS Standard Deviation of Expected matrix for all Modes each User, Ordered by the Descending of Standard Deviation of Expected matrix --- confusion matrix based on different attributes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_T = user_elt_with_errors_df.T\n",
                "df_T = df_T.rename_axis('user_id').reset_index()\n",
                "df_sorted = df_T.sort_values('all_mode_expected_SD_EC', ascending=False)\n",
                "df_sorted.reset_index(drop=True, inplace=True)\n",
                "df_sorted.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# double check attribute and matrix before store the results and plot\n",
                "attribute, matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 61,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sorted.to_csv('CSVs/' + matrix + '_' + attribute + '_diff_SD.csv')\n",
                "df_sorted = pd.read_csv('CSVs/' + matrix + '_' + attribute + '_diff_SD.csv')\n",
                "df_sorted = df_sorted.drop(['Unnamed: 0'], axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 62,
            "metadata": {},
            "outputs": [],
            "source": [
                "# pip install -U kaleido\n",
                "\n",
                "import plotly.graph_objects as go\n",
                "\n",
                "def diff_SD_plot(df, matrix, attribute, unit):\n",
                "\n",
                "    fig = go.Figure()\n",
                "    x = df.index\n",
                "    title = ''\n",
                "    y1 = df['dif_expected_user_laberd_mean'].apply(abs)\n",
                "    fig.add_trace(go.Scatter(x=x, y=y1, mode='markers', name='Difference between Expected and User labeled ' + matrix, \n",
                "                            text=df['user_id']))\n",
                "\n",
                "    y2 = df['all_mode_expected_SD_EC']\n",
                "    fig.add_trace(go.Scatter(x=x, y=y2, mode='markers', name='Standard Deviation of Expected ' + matrix,\n",
                "                            text=df['user_id']))\n",
                "\n",
                "    fig.update_layout(title='Difference between Expected and User labeled '+ matrix +' VS Standard Deviation of Expected '+ matrix +' <br /> for all Modes each User Ordered by the Descending of Standard Deviation of Expected '+ matrix +'  <br /> --- confusion matrix based on ' + attribute, #+ ' ' + unit,\n",
                "                    xaxis_title='user', yaxis_title= matrix + ' (' + unit +')' #'Energy (MWH)', lb/PMT\n",
                "                    , legend=dict(orientation='h'),\n",
                "                    width=1200,height=800)\n",
                "    fig.write_image(\"plots/\"+matrix+\"_\" + attribute + \"_diff_SD_4_folds.png\", format=\"png\")\n",
                "\n",
                "    fig.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "diff_SD_plot(df_sorted, matrix, attribute, unit)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Cumulative EC/CE by user --- confusion matrix based on different attributes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_plot = elt_with_errors_all.copy()\n",
                "grouped_df = df_plot.groupby('user_id').sum()\n",
                "grouped_df = grouped_df.reset_index()\n",
                "\n",
                "grouped_df_sorted = grouped_df.sort_values('expected', ascending=False)\n",
                "grouped_df_sorted.reset_index(drop=True, inplace=True)\n",
                "grouped_df_sorted['user_id'] = grouped_df_sorted['user_id'].apply(str)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": [
                "grouped_df_sorted.to_csv('CSVs/' + matrix + '_'+ attribute +'_user_cumulative.csv')\n",
                "\n",
                "grouped_df_sorted = pd.read_csv('CSVs/' + matrix + '_' + attribute + '_user_cumulative.csv')\n",
                "grouped_df_sorted = grouped_df_sorted.drop(['Unnamed: 0'], axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.graph_objects as go\n",
                "from plotly.subplots import make_subplots\n",
                "from plotly.offline import plot\n",
                "\n",
                "def cumulative_matrix_plot(df, matrix, attribute, unit):\n",
                "    # expected\tuser_labeled\tconfusion_var\tuser_var\tconfusion_sd\tuser_sd\n",
                "    df = df.reset_index()\n",
                "    df['index'] = df.index+1\n",
                "    # df = df[df['user_id']== uuid.UUID(user_id)]\n",
                "\n",
                "    # create plot\n",
                "    fig = go.Figure()\n",
                "\n",
                "    # Create subplots with specified row heights\n",
                "    fig = make_subplots(rows=2, cols=1, row_heights=[1, 0.3])\n",
                "\n",
                "    # Add the first trace to the first row\n",
                "    fig.add_trace(go.Bar(\n",
                "        x=df['index'],\n",
                "        y=df['expected'],\n",
                "        name='Inferred',\n",
                "        marker_color='#1f77b4',  # blue\n",
                "        error_y=dict(\n",
                "            type='data',\n",
                "            array=df['confusion_sd'],\n",
                "            visible=True,\n",
                "            color='#000000', \n",
                "            thickness=0.5  \n",
                "        )\n",
                "        # ,texttemplate='%{text}',\n",
                "        # text= df['user_id']\n",
                "    ),row=1, col=1)\n",
                "\n",
                "    # Add the second trace to the second row\n",
                "    fig.add_trace(go.Bar(\n",
                "        x=df['index'],\n",
                "        y=df['user_labeled'],\n",
                "        name='user labeled',\n",
                "        marker_color='#2ca02c',  # green\n",
                "        error_y=dict(\n",
                "            type='data',\n",
                "            array=df['user_sd'],\n",
                "            visible=True,\n",
                "            color='#000000',  \n",
                "            thickness=0.5 \n",
                "        )\n",
                "    ), row=1, col=1)\n",
                "\n",
                "    data1 = {'inferred': df['expected'].sum(), 'user labeled': df['user_labeled'].sum()}\n",
                "    df1 = pd.DataFrame(data1.items(), columns=['sum_name', 'sum_value'])\n",
                "\n",
                "    data2 = {'confusion_sd_sum': math.sqrt(df['confusion_var'].sum()), 'user_sd_sum': math.sqrt(df['user_var'].sum())}\n",
                "    df2 = pd.DataFrame(data2.items(), columns=['sd_sum_name', 'sd_sum_value'])\n",
                "\n",
                "    merged_df = pd.concat([df1, df2], axis=1)\n",
                "    title = ''\n",
                "    fig.add_trace(go.Bar(\n",
                "        x=merged_df['sum_value'],\n",
                "        y=merged_df['sum_name'],\n",
                "        marker_color=['#1f77b4', '#2ca02c'],  \n",
                "        orientation='h',\n",
                "        showlegend=False,\n",
                "        error_x=dict(\n",
                "            type='data',\n",
                "            array=merged_df['sd_sum_value'],\n",
                "            visible=True,\n",
                "            color='#000000',  \n",
                "            thickness=1  \n",
                "        ), width=[0.4, 0.4, 0.4], \n",
                "    ), row=2, col=1)\n",
                "\n",
                "\n",
                "    fig.update_layout(\n",
                "        title='Cumulative ' + matrix + ' by user --- confusion matrix based on ' + attribute, # + ' (second)',\n",
                "        xaxis_title= 'user ID',\n",
                "        yaxis_title= matrix + ' (' + unit + ')',\n",
                "        font=dict(\n",
                "            family='Arial',  \n",
                "            size=12,  \n",
                "            color='#333333'  \n",
                "        ),\n",
                "        barmode='group', \n",
                "        bargap=0.1,\n",
                "        bargroupgap=0.1\n",
                "    )\n",
                "\n",
                "    fig.update_layout(\n",
                "        width=1200,  \n",
                "        height=800 \n",
                "    )\n",
                "\n",
                "    fig.update_xaxes(title_text= matrix + ' (' + unit + ')', row=2, col=1)\n",
                "    # fig.update_yaxes(title_text=\"Cumulative energy consumption\", showgrid=False, row=2, col=1)\n",
                "\n",
                "\n",
                "    # plot(fig, filename='user_Cumulative_EC.html', auto_open=True)\n",
                "    fig.write_image(\"plots/\" + matrix + \"_\" + attribute + \"_cumulative_4_folds.png\", format=\"png\")\n",
                "\n",
                "    # Show the figure\n",
                "    fig.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cumulative_matrix_plot(grouped_df_sorted, matrix, attribute, unit)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### initial example (can ignore)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import plotly.graph_objects as go\n",
                "df = grouped_df_sorted\n",
                "expected_sum = df['expected'].sum()\n",
                "user_labeled_sum = df['user_labeled'].sum()\n",
                "confusion_sd_sum = math.sqrt(df['confusion_var'].sum())\n",
                "user_sd_sum = math.sqrt(df['user_var'].sum())\n",
                "\n",
                "summary_df = pd.DataFrame({'Metric': ['Expected', 'User Labeled', 'Confusion SD', 'User SD'],\n",
                "                           'Sum': [expected_sum, user_labeled_sum, confusion_sd_sum, user_sd_sum]})\n",
                "\n",
                "fig = go.Figure(data=[go.Bar(y=summary_df['Metric'], x=summary_df['Sum'], orientation='h')])\n",
                "\n",
                "fig.update_layout(title='Sum of Metrics', xaxis_title='Sum', yaxis_title='Metric')\n",
                "\n",
                "fig.show()\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Mode count and Mode shared by distance"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Grace's methods"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### old method "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " Sampling and probability fcns for both counts and distances"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # sampling and probability calculation fcns which you need to do for both counts and distances.\n",
                "\n",
                "# import pandas as pd\n",
                "# import numpy as np\n",
                "# from numpy.random import default_rng\n",
                "\n",
                "# '''\n",
                "# Sample a CM (as a DF) and create n # of CMs based on it.\n",
                "# input: a CM to sample, number of times to sample\n",
                "# output: a list of n CMs\n",
                "# '''\n",
                "# def sampling(samplingCM, n):\n",
                "#     # dirichlet sampling\n",
                "#     v = [] # from the CM, going left to right by row\n",
                "#     for row in samplingCM.index:\n",
                "#         for col in samplingCM.columns:\n",
                "#             v.append(samplingCM.loc[row][col])\n",
                "#     a = np.ones(samplingCM.size)\n",
                "\n",
                "#     rng = default_rng()\n",
                "#     dirichlet_samples = rng.dirichlet((v+a), n)\n",
                "\n",
                "#     # multinomial sampling\n",
                "#     multinomial_samples = []\n",
                "#     n_trips = 0\n",
                "#     for col in samplingCM:\n",
                "#         n_trips += samplingCM[col].sum()\n",
                "\n",
                "#     for params in dirichlet_samples:\n",
                "#         s = rng.multinomial(n_trips, params)\n",
                "#         multinomial_samples.append(s)\n",
                "\n",
                "#     # put each of these into their own CM, same dimensions as samplingCM (do row by row)\n",
                "#     output_CMs = []\n",
                "#     for samples in multinomial_samples:\n",
                "#         samples2D = np.reshape(samples, (len(samplingCM.index), len(samplingCM.columns)))\n",
                "#         outputCM = pd.DataFrame(samples2D, columns = samplingCM.columns, index = samplingCM.index)\n",
                "#         output_CMs.append(outputCM)\n",
                "#     return output_CMs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # '''\n",
                "# # Finding P(actual|sensed) by dividing each cell by column sum, for a list of DFs\n",
                "# # input: list of DFs of values\n",
                "# # output: list of DF of P(actual|sensed)\n",
                "# # '''\n",
                "# def actual_given_sensed_CM(list_of_value_CMs):\n",
                "#     actual_given_sensed = []\n",
                "#     for cm in list_of_value_CMs:\n",
                "#         probs = cm.div(cm.sum(axis=0), axis=1)\n",
                "#         actual_given_sensed.append(probs)\n",
                "#     return actual_given_sensed"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Count estimation function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # count estimate and variance calculation function\n",
                "\n",
                "# import pandas as pd\n",
                "# import numpy as np\n",
                "\n",
                "\n",
                "# '''\n",
                "# Finding estimated values and variances FOR COUNTS.\n",
                "# input:\n",
                "#     predicted_counts: a dictionary {\"mode1\": # of trips predicted in mode1...}\n",
                "#     actual_given_sensed: list of DFs, which have probabilities in each cell\n",
                "# output: a single estimated count for each mode, and a single estimated variance for each mode count. \n",
                "#     (prints some other interesting stuff out too)\n",
                "# '''\n",
                "# def count_estimate(predicted_counts, actual_given_sensed):\n",
                "#     # find expected counts based on each actual_given_sensed CM and predicted counts\n",
                "#     expected_counts = [] #list of dfs (one df per cm)\n",
                "#     for df in actual_given_sensed:\n",
                "#         expected_value = df.mul(pd.Series(predicted_counts), axis = 'columns') # multiply row by row\n",
                "#         expected_value = expected_value.sum(axis='columns') # sum of each row\n",
                "#         expected_counts.append(expected_value)\n",
                "\n",
                "#     # average expected values: concat dfs and find mean\n",
                "#     all_expected = pd.concat(expected_counts, axis='columns')\n",
                "#     average_ev = all_expected.mean(axis='columns')\n",
                "\n",
                "#     #VARIANCES\n",
                "#     # variance of each cell\n",
                "#     df_list = []\n",
                "#     for df in actual_given_sensed:\n",
                "#         df_list.append(df.to_numpy())\n",
                "#     cell_variance = pd.DataFrame(np.square(pd.DataFrame(np.dstack((df_list)).std(axis=2), columns = actual_given_sensed[0].columns, index = actual_given_sensed[0].index))) # calculate variance for each entry  changed\n",
                "\n",
                "#     # multiply each row of cell variances by  the row of n_i^2\n",
                "#     predicted_counts = pd.Series(predicted_counts)\n",
                "\n",
                "#     n_squared = predicted_counts ** 2\n",
                "#     # n_squared = np.square(predicted_counts) #row of n^2s\n",
                "#     n2_times_var = cell_variance.mul(n_squared, axis = 'columns') # var(ax) = (a^2)*var(x) ///changed\n",
                "\n",
                "#     # sum up rows\n",
                "#     variance = n2_times_var.sum(axis='columns') \n",
                "#     return (average_ev, variance)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Distance estimation function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 254,
            "metadata": {},
            "outputs": [],
            "source": [
                "# def sort_dict(initial_dict, order):\n",
                "#     sorted_dict = {}\n",
                "#     for ele in order:\n",
                "#         if ele not in initial_dict:\n",
                "#             sorted_dict[ele] = 0\n",
                "#         else:\n",
                "#             sorted_dict[ele] = initial_dict[ele]    \n",
                "#     return sorted_dict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # distance estimation and variance calculation fcn. very similar to the count fcn, not sure whether to combine them or not.\n",
                "\n",
                "# import pandas as pd\n",
                "# import numpy as np\n",
                "\n",
                "# '''\n",
                "# Finding estimated values and variances FOR DISTANCES.\n",
                "# input: \n",
                "#     predicted_distances: a dictionary {\"mode1\": # of trips predicted in mode1...}\n",
                "#     actual_given_sensed: list of DFs, which have probabilities in each cell\n",
                "#     os: which OS we're using, either \"ios\" or \"android\"\n",
                "# output: a single estimated distance for each mode, and a single estimated variance for each mode. \n",
                "#     (prints some other interesting stuff out too)\n",
                "# '''\n",
                "# def distance_estimate(predicted_distances, actual_given_sensed, os, sampling):\n",
                "#     os_unit_info = unit_dist_MCS_df #////changed\n",
                "\n",
                "#     # adjusting using os unit info\n",
                "#     adjusted_predicted_distances = {}\n",
                "#     for mode in predicted_distances.keys(): # e.g.mode = ('Gas Car, with others', 'android'); os = 'multiple_devices'\n",
                "#         if os == 'multiple_devices':\n",
                "#             mode_os =  mode[1]\n",
                "#             mode_name = mode[0]\n",
                "#             if  mode_os == 'ios' or mode_os == 'android':\n",
                "#                 if mode_name in adjusted_predicted_distances:\n",
                "#                     adjusted_predicted_distances[mode_name] += predicted_distances[mode] * os_unit_info[mode_os][0]\n",
                "#                 else: \n",
                "#                     adjusted_predicted_distances[mode_name] = predicted_distances[mode] * os_unit_info[mode_os][0]\n",
                "#             else:\n",
                "#                 raise Exception(\"multiple_devices: New device discovered.\")\n",
                "\n",
                "#         else:\n",
                "#             adjusted_predicted_distances[mode] = predicted_distances[mode] * os_unit_info[os][0]\n",
                "\n",
                "#     adjusted_predicted_distances = sort_dict(adjusted_predicted_distances, actual_given_sensed[0].columns)\n",
                "\n",
                "#     # find expected counts based on each actual_given_sensed CM and predicted counts\n",
                "#     expected_counts = [] #list of dfs (one df per cm)\n",
                "#     for df in actual_given_sensed:\n",
                "#         expected_value = df.mul(adjusted_predicted_distances, axis = 'columns') # multiply row by row\n",
                "#         expected_value = expected_value.sum(axis='columns') # sum of each row\n",
                "#         expected_counts.append(expected_value)\n",
                "    \n",
                "#     # average expected values: concat dfs and find mean\n",
                "#     all_expected = pd.concat(expected_counts, axis='columns')\n",
                "#     average_ev = all_expected.mean(axis='columns')\n",
                "\n",
                "#     #VARIANCES\n",
                "#     df_list = []\n",
                "#     for df in actual_given_sensed:\n",
                "#         df_list.append(df.to_numpy())\n",
                "\n",
                "#     # Note: If sampling=False, variance1 is 0. There is only one CM, so we don't have cell_variance.\n",
                "\n",
                "#     # variance of each cell in prob CMs    \n",
                "#     cell_variance = np.square(pd.DataFrame(np.dstack((df_list)).std(axis=2), columns = actual_given_sensed[0].columns, index = actual_given_sensed[0].index)) \n",
                "#     # result = np.nanvar(np.dstack(df_list), axis=2)\n",
                "#     # cell_variance = pd.DataFrame(result, columns = actual_given_sensed[0].columns, index = actual_given_sensed[0].index)\n",
                "    \n",
                "#     # multiply each row of cell variances by the row of L_i^2\n",
                "#     adjusted_predicted_distances = pd.DataFrame([adjusted_predicted_distances])\n",
                "#     n_squared = np.square(adjusted_predicted_distances) #row of L^2s\n",
                "#     variance1 = cell_variance.mul(n_squared.values, axis = 'columns') # E(L_mode)^2 V(p)\n",
                "    \n",
                "\n",
                "#     # Note: If sampling=False, avg_actual_given_sensed is the CM itself.\n",
                "\n",
                "#     # extra variance term since distance has its own uncertainty\n",
                "#     avg_actual_given_sensed = pd.DataFrame(np.dstack((df_list)).mean(axis=2), columns = actual_given_sensed[0].columns, index = actual_given_sensed[0].index)\n",
                "    \n",
                "\n",
                "#     dist_variance_dic = {}\n",
                "#     for mode in predicted_distances.keys(): # e.g.mode = ('Gas Car, with others', 'android'); os = 'multiple_devices'\n",
                "#         if os == 'multiple_devices':\n",
                "#             mode_os =  mode[1]\n",
                "#             mode_name = mode[0]\n",
                "#             if  mode_os == 'ios' or mode_os == 'android':\n",
                "#                 if mode_name in dist_variance_dic:\n",
                "#                     dist_variance_dic[mode_name] += np.square(predicted_distances[mode]) * os_unit_info[mode_os][1]\n",
                "#                 else:\n",
                "#                     dist_variance_dic[mode_name] = np.square(predicted_distances[mode]) * os_unit_info[mode_os][1] # row of L_i^2s\n",
                "#         else:\n",
                "#             dist_variance_dic[mode] = np.square(predicted_distances[mode]) * os_unit_info[os][1] # row of L_i^2s\n",
                "        \n",
                "#     dist_variance_dic = sort_dict(dist_variance_dic, avg_actual_given_sensed.columns)\n",
                "    \n",
                "#     dist_variance = pd.Series(dist_variance_dic)\n",
                "#     # dist_variance = np.square(pd.Series(predicted_distances)) * os_unit_info[os][1] # row of L_i^2s\n",
                "    \n",
                "#     variance2 = dist_variance.mul(np.square(avg_actual_given_sensed)) # E(p)^2*V(L_mode)]\n",
                "\n",
                "#     # sum up rows\n",
                "#     variance = variance1.add(variance2)\n",
                "#     variance = variance.sum(axis='columns')\n",
                "\n",
                "#     return (average_ev, variance)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "find_counts and find_distances\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # find counts \n",
                "# def find_counts(trainingCM, data, sampling):\n",
                "#     output_CMs = []\n",
                "#     if sampling:\n",
                "#         countSamples = sampling(trainingCM, 2000) # you can change 2000 to anything!\n",
                "#     else:\n",
                "#         output_CMs.append(trainingCM)\n",
                "#     countProbs = actual_given_sensed_CM(output_CMs)\n",
                "#     countEstimates = count_estimate(data, countProbs)\n",
                "#     return countEstimates\n",
                "\n",
                "# # find distances\n",
                "# def find_distances(trainingCM, data, os, sampling):\n",
                "#     output_CMs = []\n",
                "#     if sampling:\n",
                "#         output_CMs = sampling(trainingCM, 2000) # you can change 2000 to anything!\n",
                "#     else:\n",
                "#         output_CMs.append(trainingCM)\n",
                "#     distanceProbs = actual_given_sensed_CM(output_CMs)\n",
                "#     distanceEstimates = distance_estimate(data, distanceProbs, os, sampling)\n",
                "#     return distanceEstimates"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "##### new method"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Count estimation function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [],
            "source": [
                "# sort the dictionary by the order\n",
                "def sort_dict(initial_dict, order):\n",
                "    sorted_dict = {}\n",
                "    for ele in order:\n",
                "        if ele not in initial_dict:\n",
                "            sorted_dict[ele] = 0\n",
                "        else:\n",
                "            sorted_dict[ele] = initial_dict[ele]    \n",
                "    return sorted_dict"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next cell was shared from Grace"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "'''\n",
                "inputCM: a dataframe of column normalized counts (so of P(actual|predicted)), columns = sensed, rows = actual modes\n",
                "predictions: a Series of predicted counts per mode\n",
                "NMC: number of times to repeat\n",
                "'''\n",
                "\n",
                "def new_method(inputCM, predictions):\n",
                "    # get column predictions from inputCM\n",
                "    column_probabilities = inputCM\n",
                "    print(\"probabilities:\\n\", column_probabilities)\n",
                "    mean = {}\n",
                "    variance = {}\n",
                "    # calculate mean per mode\n",
                "    for index, row in column_probabilities.iterrows():\n",
                "        mean[index] = (row*predictions).sum()\n",
                "    \n",
                "    # calculate variance per mode\n",
                "    for index, row in column_probabilities.iterrows():\n",
                "        variance[index] =  (predictions*row*(1-row)).sum()\n",
                "    \n",
                "    return(pd.Series(mean), pd.Series(variance))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Mode Counts & Mode shared by distance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 72,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "def confusion_matrices_to_mode_count_distance_mean_variance(user_confusion_matrices_dic, validation_trips, matrix, sampling):\n",
                "    mean_ev_dic = {}\n",
                "    variance_dic = {}\n",
                "    combined_mean_ev_dic = {}\n",
                "    combined_variance_dic = {}\n",
                "\n",
                "    for user_id, confusion_matrix in user_confusion_matrices_dic.items():\n",
                "        print(user_id)\n",
                "        valid_trips_user = validation_trips[validation_trips['user_id']== user_id]\n",
                "        \n",
                "        if matrix=='trip_count':\n",
                "            valid_trips_user_dic_temp = valid_trips_user.groupby('mode_pred').size().to_dict()\n",
                "        elif (matrix=='distance'):\n",
                "            \n",
                "            valid_trips_user_dic_temp = valid_trips_user.groupby('mode_pred').sum()['distance'].to_dict()\n",
                "            \n",
                "            # Divide each value by 1000 and create a new dictionary\n",
                "            valid_trips_user_dic_temp_1 = {key: value / 1000 for key, value in valid_trips_user_dic_temp.items()}\n",
                "\n",
                "            # Convert the new dictionary's values to integers and create another new dictionary\n",
                "            valid_trips_user_dic_temp_2 = {key: int(value) for key, value in valid_trips_user_dic_temp_1.items()}\n",
                "\n",
                "        cm = pd.DataFrame(confusion_matrix, index= list(confusion_matrix.index))\n",
                "\n",
                "        if matrix=='trip_count':\n",
                "            valid_trips_user_dic_temp = sort_dict(valid_trips_user_dic_temp, cm.columns)\n",
                "\n",
                "            predictions = pd.Series(valid_trips_user_dic_temp)\n",
                "            (mean_ev, variance) = new_method(cm/cm.sum(axis=0), predictions)\n",
                "            \n",
                "            # mean_ev, variance = find_counts(confusion_matrix, valid_trips_user_dic_temp, sampling) # old method\n",
                "\n",
                "        elif matrix=='distance':\n",
                "            valid_trips_user_dic_temp_2 = sort_dict(valid_trips_user_dic_temp_2, cm.columns)\n",
                "\n",
                "            predictions = pd.Series(valid_trips_user_dic_temp_2)\n",
                "            (mean_ev, variance) = new_method(cm/cm.sum(axis=0), predictions)\n",
                "\n",
                "            # mean_ev, variance = find_distances(confusion_matrix, valid_trips_user_dic_temp_2, sampling)  # old method\n",
                "\n",
                "        mean_ev_d = mean_ev.to_dict()\n",
                "        variance_d = variance.to_dict()\n",
                "        mean_ev_dic[user_id] = mean_ev_d\n",
                "        variance_dic[user_id] = variance_d\n",
                "\n",
                "        for key, value in mean_ev_d.items():\n",
                "            if key in combined_mean_ev_dic:\n",
                "                combined_mean_ev_dic[key] += value\n",
                "            else:\n",
                "                combined_mean_ev_dic[key] = value\n",
                "\n",
                "        for key, value in variance_d.items():\n",
                "            if key in combined_variance_dic:\n",
                "                combined_variance_dic[key] += value\n",
                "            else:\n",
                "                combined_variance_dic[key] = value\n",
                "\n",
                "    # Save the dictionary to a file\n",
                "    with open('mean_ev_dic_' + matrix +'.pickle', 'wb') as file:\n",
                "        pickle.dump(mean_ev_dic, file)\n",
                "    \n",
                "    with open('variance_dic_' + matrix +'.pickle', 'wb') as file:\n",
                "        pickle.dump(variance_dic, file)\n",
                "    \n",
                "    with open('combined_mean_ev_dic_' + matrix +'.pickle', 'wb') as file:\n",
                "        pickle.dump(combined_mean_ev_dic, file)\n",
                "    \n",
                "    with open('combined_variance_dic_' + matrix +'.pickle', 'wb') as file:\n",
                "        pickle.dump(combined_variance_dic, file)\n",
                "        \n",
                "    return mean_ev_dic, variance_dic, combined_mean_ev_dic, combined_variance_dic\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 'matrix' can be 'trip_count' or 'distance'\n",
                "matrix = 'distance'\n",
                "sampling = False\n",
                "\n",
                "average_ev_dic, variance_dic, combined_average_ev_dic, combined_variance_dic = confusion_matrices_to_mode_count_distance_mean_variance(user_confusion_matrices_dic, validation_trips, matrix, sampling)\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The below cell is checking distrubution of mode_pred and mode_true for a special mode (can ignore)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# check distrubution of mode_pred and mode_true for a special mode \n",
                "\n",
                "validation_trip_car_alone_pred = validation_trips[validation_trips['mode_pred']=='Gas Car, drove alone']\n",
                "validation_trip_car_alone_users_pred = validation_trip_car_alone_pred.groupby('user_id').sum()\n",
                "validation_trip_car_alone_users_pred = validation_trip_car_alone_users_pred.reset_index()\n",
                "\n",
                "validation_trip_car_alone = validation_trips[validation_trips['mode_true']=='Gas Car, drove alone']\n",
                "validation_trip_car_alone_users = validation_trip_car_alone.groupby('user_id').sum()\n",
                "validation_trip_car_alone_users = validation_trip_car_alone_users.reset_index()\n",
                "\n",
                "import pandas as pd\n",
                "import plotly.express as px\n",
                "\n",
                "# Replace 'column_name' with the actual column name you want to plot\n",
                "# Assuming your DataFrame is named 'df'\n",
                "fig = px.histogram(validation_trip_car_alone_users, x='distance_miles', nbins=500)  # Adjust the number of bins as needed\n",
                "\n",
                "# Update layout if desired\n",
                "fig.update_layout(\n",
                "    xaxis_title='Values',\n",
                "    yaxis_title='Frequency',\n",
                "    title='Histogram'\n",
                ")\n",
                "\n",
                "# Show the plot\n",
                "fig.show()\n",
                "\n",
                "# Replace 'column_name' with the actual column name you want to plot\n",
                "# Assuming your DataFrame is named 'df'\n",
                "fig = px.histogram(validation_trip_car_alone_users_pred, x='distance_miles', nbins=500)  # Adjust the number of bins as needed\n",
                "\n",
                "# Update layout if desired\n",
                "fig.update_layout(\n",
                "    xaxis_title='Values',\n",
                "    yaxis_title='Frequency',\n",
                "    title='Histogram'\n",
                ")\n",
                "\n",
                "# Show the plot\n",
                "fig.show()\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_df(dic_df, expected_value):\n",
                "    dic_df = dic_df.T\n",
                "    dic_df.reset_index(inplace=True)\n",
                "    dic_df.rename(columns={'index': 'user_id'}, inplace=True)\n",
                "    # df: user_id\tpredicted_true_mode\texpected_distance\n",
                "    new_df = pd.DataFrame(columns=['user_id', 'mode', expected_value])\n",
                "    for _, row in dic_df.iterrows():\n",
                "        user = row['user_id']\n",
                "        for column in dic_df.columns:\n",
                "                if column != 'user_id' and pd.notnull(row[column]):\n",
                "                    predicted_true_mode = column\n",
                "                    expected_matrix = row[column]\n",
                "                    \n",
                "                    new_df = new_df.append({'user_id': user, 'mode': predicted_true_mode, expected_value: expected_matrix}, ignore_index=True)\n",
                "    return new_df     \n",
                "\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# check the matrix\n",
                "matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# expected_true_df contains the 'user_id', 'mode', 'expected_distance' and 'variance_distance'\n",
                "\n",
                "variance_dic_df = pd.DataFrame(pd.read_pickle('variance_dic_' + matrix +'.pickle'))\n",
                "variance_dic_df = clean_df(variance_dic_df,'variance_distance')\n",
                "\n",
                "expected_dic_df = pd.DataFrame(pd.read_pickle('mean_ev_dic_' + matrix +'.pickle'))\n",
                "expected_dic_df = clean_df(expected_dic_df,'expected_distance')\n",
                "\n",
                "expected_true_df = pd.merge(expected_dic_df, variance_dic_df, on=['user_id', 'mode'])\n",
                "expected_true_df\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# In validation dataset, get the sum distance of each mode of each user \n",
                "valid_mode_true_distance = pd.DataFrame(validation_trips.groupby(['user_id', 'mode_true']).sum()['distance_km'])\n",
                "valid_mode_true_distance = valid_mode_true_distance.reset_index()\n",
                "valid_mode_true_distance.rename(columns={'mode_true': 'mode'}, inplace=True)\n",
                "valid_mode_true_distance.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [],
            "source": [
                "# calculate the accuracy based on the confusion_matrix.\n",
                "# input: confusion_matrix, each row is true values, each column is predicted values\n",
                "def get_accuracy(confusion_matrix):\n",
                "    column_predictions = pd.DataFrame(confusion_matrix, index= list(confusion_matrix.index))\n",
                "\n",
                "    # Initialize the sum of correct predictions (where row index matches the column)\n",
                "    correct_sum = 0\n",
                "\n",
                "    # Iterate over each row and each column\n",
                "    for index, row in column_predictions.iterrows():\n",
                "        for column_name, value in row.items():\n",
                "            # Check if the row index matches the column name\n",
                "            if index == column_name:\n",
                "                correct_sum += value\n",
                "                break\n",
                "\n",
                "    # Calculate the total sum of all values in the DataFrame\n",
                "    total_sum = column_predictions.to_numpy().sum()\n",
                "\n",
                "    # Calculate accuracy by dividing correct_sum by total_sum\n",
                "    accuracy = correct_sum / total_sum\n",
                "    print(\"Accuracy:\", accuracy)\n",
                "    return accuracy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#  If we want to calculate the accuracy of trip mode, we need to check the mode_true and mode_pred\n",
                "#  Get the accuracy of each user based on the ground truth value and predicted value\n",
                "def get_user_accuracyDf_from_CM(dataset_trips, column_name):\n",
                "    dataset_trips_user_id_list = list(dataset_trips['user_id'].unique())\n",
                "\n",
                "    user_dataset_CM_dic = {}\n",
                "    user_dataset_accuracy_CM_dic = {}\n",
                "    for user_id in dataset_trips_user_id_list:\n",
                "        user_dataset_trips = dataset_trips[dataset_trips['user_id'] == user_id]\n",
                "        \n",
                "        predicted_values = user_dataset_trips['mode_pred']\n",
                "        true_values = user_dataset_trips['mode_true']\n",
                "        sample_weight = user_dataset_trips['distance']\n",
                "\n",
                "        confusion_matrix = pd.crosstab(true_values, predicted_values, sample_weight, aggfunc='sum') # weight: trip duration\n",
                "        confusion_matrix[confusion_matrix.isnull()] = 0\n",
                "        accuracy = get_accuracy(confusion_matrix)\n",
                "        user_dataset_accuracy_CM_dic[user_id] = accuracy\n",
                "        user_dataset_CM_dic[user_id] = confusion_matrix\n",
                "        \n",
                "        \n",
                "    sorted_user_dataset_accuracy_CM_dic = dict(sorted(user_dataset_accuracy_CM_dic.items(), key=lambda item: item[1], reverse=True))\n",
                "    sorted_user_dataset_accuracy_CM_df = pd.DataFrame(data = sorted_user_dataset_accuracy_CM_dic, index = [0])\n",
                "    sorted_user_dataset_accuracy_CM_df = sorted_user_dataset_accuracy_CM_df.T.reset_index()\n",
                "    sorted_user_dataset_accuracy_CM_df = sorted_user_dataset_accuracy_CM_df.rename(columns = {'index':'user_id', 0:column_name})\n",
                "    return sorted_user_dataset_accuracy_CM_df\n",
                "\n",
                "sorted_user_testing_accuracy_CM_df = get_user_accuracyDf_from_CM(test_trips,'model_testing_prediction_accuracy')\n",
                "sorted_user_validation_accuracy_CM_df = get_user_accuracyDf_from_CM(validation_trips,'model_validation_prediction_accuracy')\n",
                "sorted_user_validation_accuracy_CM_list = list(sorted_user_validation_accuracy_CM_df['user_id'])\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# accuracy_GT_ED is the accuracy of predicted mode distance, not the accuracy of trip mode.\n",
                "# If we want to calculate the accuracy of trip mode, we need to check the mode_true and mode_pred\n",
                "\n",
                "GT_mode_df = pd.DataFrame({})\n",
                "user_id_list = list(expected_true_df.user_id.unique())\n",
                "\n",
                "for user_id in user_id_list:\n",
                "    user_expected_true_df = expected_true_df[expected_true_df['user_id'] == user_id]\n",
                "    user_valid_mode_true_distance = valid_mode_true_distance[valid_mode_true_distance['user_id'] == user_id]\n",
                "    df_result = user_expected_true_df.merge(user_valid_mode_true_distance[['mode','distance_km']], on='mode', how='outer')\n",
                "    df_result = df_result.replace(np. nan,0) \n",
                "    df_result['user_id'] = user_id\n",
                "    df_result['diff_GT_ED'] = abs(df_result['expected_distance'] - df_result['distance_km'])\n",
                "    GT_mode_df= pd.concat([GT_mode_df, df_result], axis=0)\n",
                "\n",
                "GT_mode_df_users = pd.DataFrame(GT_mode_df.groupby('user_id').sum())\n",
                "GT_mode_df_users = GT_mode_df_users.reset_index()\n",
                "\n",
                "GT_mode_df_users['accuracy_GT_ED'] = 1-(GT_mode_df_users['diff_GT_ED'] / GT_mode_df_users['distance_km']) \n",
                "GT_mode_df_sorted = GT_mode_df_users.sort_values('accuracy_GT_ED', ascending=False)\n",
                "GT_mode_df_sorted.reset_index(drop=True, inplace=True)\n",
                "\n",
                "user_id_list_basedOnAccuracy = list(GT_mode_df_sorted['user_id'])\n",
                "\n",
                "GT_mode_df_sorted.head(10)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sort the df based on the accuracy of user's validation dataset\n",
                "expected_true_df['user_id'] = expected_true_df['user_id'].astype('category')\n",
                "expected_true_df['user_id'].cat.reorder_categories(sorted_user_validation_accuracy_CM_list, inplace=True)\n",
                "expected_true_df.sort_values('user_id', inplace= True)\n",
                "expected_true_df.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a function to calculate the square root\n",
                "def calculate_square_root(x):\n",
                "    return x ** 0.5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_result_text = expected_true_df.merge(GT_mode_df_sorted[['user_id','accuracy_GT_ED']], on='user_id', how='left')\n",
                "df_result_text = df_result_text.merge(sorted_user_validation_accuracy_CM_df[['user_id','model_validation_prediction_accuracy']], on='user_id', how='left')\n",
                "df_result_text = df_result_text.merge(sorted_user_testing_accuracy_CM_df[['user_id','model_testing_prediction_accuracy']], on='user_id', how='left')\n",
                "df_result_text['SD_distance'] = df_result_text['variance_distance'].apply(calculate_square_root)\n",
                "\n",
                "df_result_text.tail(10)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Columns:\n",
                "\n",
                "1. 'expected_distance': the expected distance of the trip\n",
                "2. 'variance_distance': the variance of the expected distance\n",
                "3. 'accuracy_GT_ED': 1 - abs(ground truth distance - expected distance) / ground truth distance\n",
                "4. 'model_validation_prediction_accuracy': the accuracy of the validation dataset # Calculation: (TP+TN)/ALL\n",
                "5. 'model_testing_prediction_accuracy': the accuracy of the testing dataset # Calculation: (TP+TN)/ALL\n",
                "6. 'SD_distance': the standard deviation of the expected distance\n",
                "7. 'distance_km': the ground truth distance in kilometer\n",
                "8. 'SD_Count': abs(ground truth distance - expected distance) / standard deviation of the expected distance\n",
                "9. 'var_Count': abs(ground truth distance - expected distance) / variance of the expected distance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_result_text2 = df_result_text.merge(GT_mode_df[['user_id','distance_km','mode']], on=['user_id','mode'], how='left')\n",
                "\n",
                "df_result_text2['SD_Count'] = abs(df_result_text2['expected_distance']-df_result_text2['distance_km'])/df_result_text2['SD_distance']\n",
                "df_result_text2['var_Count'] = abs(df_result_text2['expected_distance']-df_result_text2['distance_km'])/df_result_text2['variance_distance']\n",
                "\n",
                "# some rows with variance_distance == 0, so the SD_Count came to \"inf\".\n",
                "# Now if the 'variance_distance' == 0, we set the SD_Count and var_Count to 0\n",
                "df_result_text2['SD_Count'] = df_result_text2.apply(lambda row: 0 if row['variance_distance'] == 0 else row['SD_Count'], axis=1)\n",
                "df_result_text2['var_Count'] = df_result_text2.apply(lambda row: 0 if row['variance_distance'] == 0 else row['var_Count'], axis=1)\n",
                "df_result_text2.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 97,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_result_text3 = df_result_text2[df_result_text2['SD_distance']>1]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import plotly.express as px\n",
                "\n",
                "# Create the DataFrame\n",
                "df = df_result_text2\n",
                "\n",
                "# Sort the DataFrame by 'Accuracy' column in ascending order\n",
                "df = df.sort_values(by='model_validation_prediction_accuracy', ascending=False)\n",
                "\n",
                "# Create the box plot using Plotly\n",
                "fig = px.box(df, x='model_validation_prediction_accuracy', y='var_Count', color='user_id', points='all')\n",
                "\n",
                "# Update the layout\n",
                "fig.update_layout(\n",
                "    xaxis=dict(autorange='reversed'),\n",
                "    xaxis_title='Accuracy',\n",
                "    yaxis_title='variance_Count',\n",
                "    title='Box Plot of the counts of variance of expected distance  <br /> from the expected distance to the ground truth distance for different Users and Modes',\n",
                "    legend_title='User'\n",
                ")\n",
                "\n",
                "# Show the plot\n",
                "fig.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import plotly.express as px\n",
                "\n",
                "# Create the DataFrame\n",
                "df = df_result_text3\n",
                "\n",
                "# Sort the DataFrame by 'Accuracy' column in ascending order\n",
                "df = df.sort_values(by='model_validation_prediction_accuracy', ascending=False)\n",
                "\n",
                "# Create the box plot using Plotly\n",
                "fig = px.box(df, x='model_validation_prediction_accuracy', y='var_Count', color='user_id', points='all')\n",
                "\n",
                "# Update the layout\n",
                "fig.update_layout(\n",
                "    xaxis=dict(autorange='reversed'),\n",
                "    xaxis_title='Accuracy',\n",
                "    yaxis_title='variance_Count',\n",
                "    title='Plot of the counts of variance of expected distance  <br /> from the expected distance to the ground truth distance for different Modes and Users with n*n confusion matrix <br /> (standard diviation > 1 data points)',\n",
                "    legend_title='User'\n",
                ")\n",
                "\n",
                "# Show the plot\n",
                "fig.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import plotly.express as px\n",
                "\n",
                "# Create the DataFrame\n",
                "df = df_result_text2\n",
                "\n",
                "# Sort the DataFrame by 'Accuracy' column in ascending order\n",
                "df = df.sort_values(by='model_validation_prediction_accuracy', ascending=False)\n",
                "\n",
                "# Create the box plot using Plotly\n",
                "fig = px.box(df, x='model_validation_prediction_accuracy', y='SD_Count', color='user_id', points='all')\n",
                "\n",
                "# Update the layout\n",
                "fig.update_layout(\n",
                "    xaxis=dict(autorange='reversed'),\n",
                "    xaxis_title='Accuracy',\n",
                "    yaxis_title='SD_Count',\n",
                "    title='Plot of the counts of standard deviation of expected distance  <br /> from the expected distance to the ground truth distance for different Users and Modes',\n",
                "    legend_title='User'\n",
                ")\n",
                "\n",
                "# Show the plot\n",
                "fig.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 101,
            "metadata": {},
            "outputs": [],
            "source": [
                "validation_trips_mode_true_dic =  validation_trips.groupby('mode_true').size().to_dict()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 102,
            "metadata": {},
            "outputs": [],
            "source": [
                "validation_trips_mode_true_dic =  validation_trips.groupby('mode_true').sum()['distance_km'].to_dict()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 103,
            "metadata": {},
            "outputs": [],
            "source": [
                "def drop_mode_except_in_list(dictionary, except_list):\n",
                "    keys = list(dictionary.keys())\n",
                "    dictionary['others'] = 0\n",
                "\n",
                "    for key in keys:\n",
                "        if key not in except_list:\n",
                "            dictionary['others'] += dictionary[key]\n",
                "            dictionary.pop(key)\n",
                "\n",
                "# only keep the mode in the energy_intensity.csv, otherwise count in 'others'\n",
                "drop_mode_except_in_list(combined_average_ev_dic, list(df_EI['mode']))\n",
                "drop_mode_except_in_list(combined_variance_dic, list(df_EI['mode']))\n",
                "drop_mode_except_in_list(validation_trips_mode_true_dic, list(df_EI['mode']))            "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.graph_objects as go\n",
                "\n",
                "# Example dictionaries\n",
                "dict1 = combined_average_ev_dic\n",
                "dict2 = validation_trips_mode_true_dic\n",
                "dict3 = {key:val for key,val in combined_variance_dic.items()}\n",
                "\n",
                "# Merge dictionaries and keep only the common keys\n",
                "common_keys = set(dict1.keys()).intersection(dict2.keys()).intersection(dict3.keys())\n",
                "merged_dict = {key: (dict1[key], dict2[key], dict3[key]) for key in common_keys}\n",
                "\n",
                "# Sort the merged_dict\n",
                "merged_dict = dict(sorted(merged_dict.items(), key=lambda x:x[1], reverse=True))\n",
                "\n",
                "# Extract keys and values from the merged dictionary\n",
                "keys = list(merged_dict.keys())\n",
                "values_dict1 = [merged_dict[key][0] for key in keys]\n",
                "values_dict2 = [merged_dict[key][1] for key in keys]\n",
                "values_dict3 = [merged_dict[key][2] for key in keys]\n",
                "\n",
                "\n",
                "# Create bar traces for each dictionary\n",
                "trace1 = go.Bar(x=keys, y=values_dict1, name='Expected mode count',\n",
                "                error_y=dict(\n",
                "                type='data',\n",
                "                array=values_dict3,\n",
                "                visible=True,\n",
                "                color='#000000',\n",
                "                thickness=0.5\n",
                "            ))\n",
                "trace2 = go.Bar(x=keys, y=values_dict2, name='Ground truth mode count')\n",
                "\n",
                "# Create the layout\n",
                "layout = go.Layout(\n",
                "    title='Expected mode count VS Ground truth mode count',\n",
                "    xaxis=dict(title='Mode name'),\n",
                "    yaxis=dict(title='Mode count'),\n",
                "    barmode='group'\n",
                ")\n",
                "\n",
                "# Create the figure and add the traces\n",
                "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
                "fig.update_layout(width = 900, height = 600)\n",
                "\n",
                "fig.write_image(\"plots/Expected mode count VS Ground truth mode count --- with one variance.png\", format=\"png\")\n",
                "\n",
                "# Show the figure\n",
                "fig.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.graph_objects as go\n",
                "\n",
                "# Example dictionaries\n",
                "dict1 = combined_average_ev_dic\n",
                "dict2 = validation_trips_mode_true_dic\n",
                "dict3 = {key:math.sqrt(val) for key,val in combined_variance_dic.items()}\n",
                "\n",
                "# Merge dictionaries and keep only the common keys\n",
                "common_keys = set(dict1.keys()).intersection(dict2.keys()).intersection(dict3.keys())\n",
                "merged_dict = {key: (dict1[key], dict2[key], dict3[key]) for key in common_keys}\n",
                "\n",
                "# Sort the merged_dict\n",
                "merged_dict = dict(sorted(merged_dict.items(), key=lambda x:x[1], reverse=True))\n",
                "\n",
                "# Extract keys and values from the merged dictionary\n",
                "keys = list(merged_dict.keys())\n",
                "values_dict1 = [merged_dict[key][0] for key in keys]\n",
                "values_dict2 = [merged_dict[key][1] for key in keys]\n",
                "values_dict3 = [merged_dict[key][2] for key in keys]\n",
                "\n",
                "\n",
                "# Create bar traces for each dictionary\n",
                "trace1 = go.Bar(x=keys, y=values_dict1, name='Expected mode shared by distance',\n",
                "                error_y=dict(\n",
                "                type='data',\n",
                "                array=values_dict3,\n",
                "                visible=True,\n",
                "                color='#000000',\n",
                "                thickness=0.5\n",
                "            ))\n",
                "trace2 = go.Bar(x=keys, y=values_dict2, name='Ground truth mode shared by distance')\n",
                "\n",
                "# Create the layout\n",
                "layout = go.Layout(\n",
                "    title='Expected mode share by distance VS Ground truth mode shared by distance   <br /> --- with one standard deviation error bar',\n",
                "    xaxis=dict(title='Mode name'),\n",
                "    yaxis=dict(title='Distance'),\n",
                "    barmode='group'\n",
                ")\n",
                "\n",
                "# Create the figure and add the traces\n",
                "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
                "fig.update_layout(width = 900, height = 600)\n",
                "\n",
                "fig.write_image(\"plots/Expected mode shared by distance VS Ground truth mode shared by distance --- with one standard deviation.png\", format=\"png\")\n",
                "\n",
                "# Show the figure\n",
                "fig.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.graph_objects as go\n",
                "\n",
                "# Example dictionaries\n",
                "dict1 = combined_average_ev_dic\n",
                "dict2 = validation_trips_mode_true_dic\n",
                "dict3 = {key:val for key,val in combined_variance_dic.items()}\n",
                "\n",
                "# Merge dictionaries and keep only the common keys\n",
                "common_keys = set(dict1.keys()).intersection(dict2.keys()).intersection(dict3.keys())\n",
                "merged_dict = {key: (dict1[key], dict2[key], dict3[key]) for key in common_keys}\n",
                "\n",
                "# Sort the merged_dict\n",
                "merged_dict = dict(sorted(merged_dict.items(), key=lambda x:x[1], reverse=True))\n",
                "\n",
                "# Extract keys and values from the merged dictionary\n",
                "keys = list(merged_dict.keys())\n",
                "values_dict1 = [merged_dict[key][0] for key in keys]\n",
                "values_dict2 = [merged_dict[key][1] for key in keys]\n",
                "values_dict3 = [merged_dict[key][2] for key in keys]\n",
                "\n",
                "\n",
                "# Create bar traces for each dictionary\n",
                "trace1 = go.Bar(x=keys, y=values_dict1, name='Expected mode shared by distance',\n",
                "                error_y=dict(\n",
                "                type='data',\n",
                "                array=values_dict3,\n",
                "                visible=True,\n",
                "                color='#000000',\n",
                "                thickness=0.5\n",
                "            ))\n",
                "trace2 = go.Bar(x=keys, y=values_dict2, name='Ground truth mode shared by distance')\n",
                "\n",
                "# Create the layout\n",
                "layout = go.Layout(\n",
                "    title='Expected mode share by distance VS Ground truth mode shared by distance <br /> --- with one variance error bar)',\n",
                "    xaxis=dict(title='Mode name'),\n",
                "    yaxis=dict(title='Distance'),\n",
                "    barmode='group'\n",
                ")\n",
                "\n",
                "# Create the figure and add the traces\n",
                "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
                "fig.update_layout(width = 900, height = 600)\n",
                "\n",
                "fig.write_image(\"plots/Expected mode shared by distance VS Ground truth mode shared by distance --- with one variance_NN.png\", format=\"png\")\n",
                "\n",
                "# Show the figure\n",
                "fig.show()\n"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "73ac5b45931ab4dd3f8e07a8d0e5daf0146eed4821bf42374f6ac6fa4af28c83"
        },
        "kernelspec": {
            "display_name": "Python 3.7.12 ('emission-private-eval')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
